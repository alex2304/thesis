{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "\n",
    "from utils.nltk_utils import *\n",
    "from utils.ngrams import *\n",
    "from utils.eval import get_test_keywords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.grammar_utils import tags_seq_to_symbols\n",
    "from utils.io import Cache\n",
    "from utils.ngrams import ngram2str\n",
    "\n",
    "\n",
    "observed_tags = Cache.load_observed_tags()\n",
    "terminal_rules = Cache.load_terminal_rules()\n",
    "\n",
    "\n",
    "def parse_phrases(tt_ngrams) -> Tuple[List, List[List[Tuple]]]:\n",
    "    global observed_tags, terminal_rules\n",
    "    \n",
    "    phrases = []\n",
    "    phrases_types = []\n",
    "\n",
    "    if observed_tags is None:\n",
    "        observed_tags = dict()\n",
    "\n",
    "    for tt_gram in tt_ngrams:\n",
    "        symbols = tuple(tags_seq_to_symbols([tag\n",
    "                                             for _, tag in tt_gram]))\n",
    "\n",
    "        phrase = tt_gram\n",
    "\n",
    "        # check if tags phrase has been already observed\n",
    "        tags_str = ngram2str(symbols)\n",
    "\n",
    "        if tags_str in observed_tags:\n",
    "            if observed_tags[tags_str] is not None:\n",
    "                phrases.append(phrase)\n",
    "                phrases_types.append(observed_tags[tags_str])\n",
    "\n",
    "            continue\n",
    "\n",
    "        p_types_dict = terminal_rules.get(tags_str)\n",
    "\n",
    "        if p_types_dict:\n",
    "            p_type = max(p_types_dict, key=lambda k: p_types_dict[k])\n",
    "\n",
    "            phrases.append(phrase)\n",
    "            phrases_types.append(p_type)\n",
    "\n",
    "            observed_tags[tags_str] = p_type\n",
    "\n",
    "        else:\n",
    "            observed_tags[tags_str] = None\n",
    "\n",
    "#     try:\n",
    "#         Cache.save_observed_tags(observed_tags)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc(e)\n",
    "\n",
    "    return phrases_types, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents() + brown.sents() + reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 210608\n",
      "Words: 5503894\n",
      "Vocab: 87046\n"
     ]
    }
   ],
   "source": [
    "sents = [tuple(s) for s in sents]\n",
    "\n",
    "words = [w.lower() \n",
    "         for s in sents for w in s]\n",
    "\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "print('Sents:', len(sents))\n",
    "print('Words:', len(words))\n",
    "print('Vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indexes\n",
    "\n",
    "# {sentence: sent_index}\n",
    "sents_ind = {sents[i]: i for i in range(len(sents))}\n",
    "\n",
    "# {word: word_index}\n",
    "vocab_ind = {vocab[j]: j for j in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping from words to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: 58858\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in vocab]\n",
    "\n",
    "# {word_index: stem}\n",
    "stems_map = {vocab_ind[word]: stem \n",
    "             for word, stem in zip(vocab, stems)}\n",
    "\n",
    "print('Stems:', len(set(stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2word = {stem: vocab[ind]\n",
    "             for ind, stem in stems_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = gensim.models.Word2Vec.load('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_stemmed = [[stems_map[vocab_ind[w.lower()]] for w in s] for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sents_stemmed, size=300, window=15, min_count=1, hs=1, negative=0)\n",
    "model.save('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.3553354740142822), ('daughter', 0.35418325662612915), ('esther', 0.3361766040325165), ('husband', 0.3246108889579773), ('absalom', 0.3200189769268036), ('mordecai', 0.31594496965408325), ('samaria', 0.3088769316673279), ('vashti', 0.30666205286979675), ('hebron', 0.298782080411911), ('selleth', 0.29791125655174255)]\n",
      "0.20989265750352995\n",
      "bring\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "print(model.wv.most_similar(positive=[stemmer.stem('woman'), stemmer.stem('king')], negative=[stemmer.stem('man')]))\n",
    "print(model.wv.similarity(stemmer.stem('campus'), stemmer.stem('dormitory')))\n",
    "print(model.wv.doesnt_match([stemmer.stem(w) for w in \"dormitory bring campus\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='rb') as fp:\n",
    "    stems_phrases = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = NGramsParser()\n",
    "\n",
    "sents_words_indexes = parser.parse_sents_tokens(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210608it [06:48, 515.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# {stem: Set[sentence_ind]}\n",
    "stems_phrases = defaultdict(dict)\n",
    "\n",
    "for i, (s_words, words_indexes) in tqdm(enumerate(zip(sents, sents_words_indexes), start=1)):\n",
    "    words_ttokens = nltk.pos_tag([w.lower() for w in s_words])\n",
    "    \n",
    "    tt_ngrams = [ngr\n",
    "                 for i in range(2, 5 + 1) \n",
    "                 for ngr in n_grams(words_ttokens, i, words_indexes, pad_left=False)]\n",
    "\n",
    "    types, phrases = parse_phrases(tt_ngrams)\n",
    "\n",
    "    # format and store phrases\n",
    "    for t, p in zip(types, phrases):\n",
    "        phrase_inds = tuple(vocab_ind[token] for token, _ in p)\n",
    "        \n",
    "        for word_ind in phrase_inds:\n",
    "            if vocab[word_ind] not in stop_words:\n",
    "                stem = stems_map[word_ind]\n",
    "                \n",
    "                if not stems_phrases[stem].get(t):\n",
    "                    stems_phrases[stem][t] = defaultdict(set)\n",
    "                \n",
    "                stem_phr_t = stems_phrases[stem][t]\n",
    "                \n",
    "                stem_phr_t[phrase_inds].add(sents_ind[s_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJP': defaultdict(set,\n",
       "             {(5566, 8631): {53379},\n",
       "              (6509, 8629): {155524},\n",
       "              (8629, 8322): {100786, 136405, 136436},\n",
       "              (8629, 10767): {142473},\n",
       "              (8629, 14426): {129262, 163315, 166916, 188102},\n",
       "              (8629, 31882): {128992},\n",
       "              (8629, 40323): {107429, 118657, 129508, 132871, 133063, 133149},\n",
       "              (8629, 77432): {103574,\n",
       "               129208,\n",
       "               129583,\n",
       "               129699,\n",
       "               130147,\n",
       "               130238,\n",
       "               130557,\n",
       "               131722,\n",
       "               133059,\n",
       "               133415,\n",
       "               134962,\n",
       "               136578,\n",
       "               143528,\n",
       "               155524,\n",
       "               164946},\n",
       "              (10042, 8629): {122610},\n",
       "              (16046, 8629): {136027},\n",
       "              (22386, 8629): {163315, 166916},\n",
       "              (28806, 8631): {78913},\n",
       "              (54085, 8631): {188105},\n",
       "              (54928, 8631): {131253},\n",
       "              (75026, 8629): {62560},\n",
       "              (76214, 8629): {134962},\n",
       "              (79104, 8629): {133063}}),\n",
       " 'ADVP': defaultdict(set,\n",
       "             {(8628, 77456): {191787},\n",
       "              (70362, 8628): {191787},\n",
       "              (77542, 8630): {89778},\n",
       "              (77736, 8630): {195255}}),\n",
       " 'NP': defaultdict(set,\n",
       "             {(1677, 53178, 8630): {195818},\n",
       "              (4142, 79104, 8629): {133063},\n",
       "              (6425, 8628): {188476},\n",
       "              (6841, 8629): {67418, 129544, 141032},\n",
       "              (6841, 8629, 5870): {67418},\n",
       "              (6841, 8629, 5870, 54834): {67418},\n",
       "              (6841, 8629, 40042): {129544},\n",
       "              (6841, 8629, 40042, 82795): {129544},\n",
       "              (6841, 8629, 52570): {141032},\n",
       "              (6958, 6841, 8629, 40042): {129544},\n",
       "              (8628, 4142): {10854,\n",
       "               90541,\n",
       "               91285,\n",
       "               97039,\n",
       "               102810,\n",
       "               124984,\n",
       "               128804,\n",
       "               129251,\n",
       "               146735,\n",
       "               192731,\n",
       "               210054},\n",
       "              (8628, 4425): {167315},\n",
       "              (8628, 6620): {188470, 188472},\n",
       "              (8628, 6841): {136474},\n",
       "              (8628, 7409): {117845, 188476},\n",
       "              (8628, 24096): {179547},\n",
       "              (8628, 43651): {133304},\n",
       "              (8628, 48292): {85693},\n",
       "              (8628, 53631): {133114},\n",
       "              (8628, 54885): {123829},\n",
       "              (8628, 72194): {166865},\n",
       "              (8628, 77432): {97073,\n",
       "               104759,\n",
       "               105355,\n",
       "               116435,\n",
       "               120802,\n",
       "               121453,\n",
       "               124985,\n",
       "               125318,\n",
       "               128875,\n",
       "               129190,\n",
       "               129706,\n",
       "               131994,\n",
       "               132229,\n",
       "               132819,\n",
       "               132875,\n",
       "               134690,\n",
       "               135168,\n",
       "               142539,\n",
       "               150797,\n",
       "               163081,\n",
       "               181475},\n",
       "              (8628, 77456): {187,\n",
       "               66413,\n",
       "               78058,\n",
       "               81366,\n",
       "               115127,\n",
       "               120880,\n",
       "               125006,\n",
       "               131072,\n",
       "               133693,\n",
       "               133696,\n",
       "               144948,\n",
       "               157951,\n",
       "               159366,\n",
       "               178610,\n",
       "               182963,\n",
       "               197604,\n",
       "               200077,\n",
       "               203270,\n",
       "               203391},\n",
       "              (8628, 77604): {85527},\n",
       "              (8628, 78866): {70195, 74759},\n",
       "              (8629, 2335): {158434},\n",
       "              (8629, 5870): {67418},\n",
       "              (8629, 5870, 54834): {67418},\n",
       "              (8629, 9098): {131420},\n",
       "              (8629, 19116): {136027},\n",
       "              (8629, 19830): {186527},\n",
       "              (8629, 20598): {179758},\n",
       "              (8629, 36199): {193280},\n",
       "              (8629, 40042): {129544},\n",
       "              (8629, 40042, 82795): {129544},\n",
       "              (8629, 52570): {141032},\n",
       "              (8629, 65152): {127306},\n",
       "              (8630, 72194): {89778},\n",
       "              (8630, 72194, 55785): {89778},\n",
       "              (8630, 72194, 55785, 38739, 32130): {89778},\n",
       "              (8631, 5414): {129133, 136426},\n",
       "              (8631, 6620): {188480},\n",
       "              (8631, 7074, 19629): {112714},\n",
       "              (8631, 9098): {87054},\n",
       "              (8631, 15479): {178064},\n",
       "              (8631, 15479, 41682): {178064},\n",
       "              (8631, 19905): {205379},\n",
       "              (8631, 20059): {191693},\n",
       "              (8631, 20059, 60864): {191693},\n",
       "              (8631, 21209): {180864},\n",
       "              (8631, 21209, 54955): {180752},\n",
       "              (8631, 29071, 19830): {125048},\n",
       "              (8631, 29135): {206542},\n",
       "              (8631, 35292): {82960},\n",
       "              (8631, 40194): {163802},\n",
       "              (8631, 48669): {204740},\n",
       "              (8631, 48669, 19116): {204740},\n",
       "              (8631, 54955): {180364, 203646, 209266},\n",
       "              (8631, 54955, 60864): {180364, 203646, 209266},\n",
       "              (8631, 65152): {128765},\n",
       "              (8631, 72837): {129581},\n",
       "              (8631, 77571, 28255): {129118},\n",
       "              (16046, 8629, 19116): {136027},\n",
       "              (28852, 51453, 8631): {82960},\n",
       "              (28852, 51453, 8631, 35292): {82960},\n",
       "              (30153, 8630): {124887},\n",
       "              (49851, 8628): {81366},\n",
       "              (51453, 8631): {82960},\n",
       "              (51453, 8631, 35292): {82960},\n",
       "              (53178, 8630): {195818},\n",
       "              (53251, 8628): {78058},\n",
       "              (53261, 73393, 8631): {128765},\n",
       "              (54085, 8628): {97073, 117626, 192731},\n",
       "              (54194, 8628): {181475},\n",
       "              (62471, 8628): {102810},\n",
       "              (65947, 8631): {108029},\n",
       "              (68158, 8630): {97769},\n",
       "              (69369, 8628): {85348},\n",
       "              (71047, 8628): {86143},\n",
       "              (73393, 8631): {128765},\n",
       "              (73393, 8631, 65152): {128765},\n",
       "              (77432, 8629): {129262},\n",
       "              (77432, 12992, 8629): {85266},\n",
       "              (77456, 41451, 8629): {204205},\n",
       "              (77456, 53261, 73393, 8631): {128765},\n",
       "              (77456, 62432, 8629): {79529},\n",
       "              (77456, 82556, 8629): {136936},\n",
       "              (77791, 8629): {118657},\n",
       "              (78030, 8628): {136474},\n",
       "              (80827, 6841, 8629, 52570): {141032},\n",
       "              (85626, 6841, 8629, 5870): {67418}}),\n",
       " 'VP': defaultdict(set,\n",
       "             {(8628, 361, 47368): {158610},\n",
       "              (8629, 6352): {153680},\n",
       "              (8631, 31882): {79580},\n",
       "              (8631, 77432): {108029, 129016, 132751, 177963}})}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_phrases(word, stemmed=True):\n",
    "    if stemmed:\n",
    "        stem = word\n",
    "    else:\n",
    "        stem = stemmer.stem(word)\n",
    "        \n",
    "    phrs = stems_phrases[stem]\n",
    "    \n",
    "    if not phrs:\n",
    "        return {}\n",
    "    \n",
    "    phrases_ = defaultdict(set)\n",
    "    \n",
    "    for p_type, phr_dict in phrs.items():\n",
    "        for phrase, sents in phr_dict.items():\n",
    "            phrase_ = tuple(vocab[ind] for ind in phrase)\n",
    "            \n",
    "            phrases_[p_type].add(phrase_)\n",
    "            \n",
    "    return phrases_\n",
    "\n",
    "stems_phrases['assum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='wb') as fp:\n",
    "    pickle.dump(stems_phrases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for analysis\n",
    "\n",
    "phrases_count = defaultdict(dict)\n",
    "\n",
    "phrases_lengths_count = defaultdict(int)\n",
    "phrases_types_count = defaultdict(int)\n",
    "\n",
    "stems_phrases_count = defaultdict(dict)\n",
    "\n",
    "for stem, stem_phrases_dict in stems_phrases.items():\n",
    "    for phrase_type, phrases_dict in stem_phrases_dict.items():\n",
    "#         stem_phrase_type_count = sum([len(s_ids) for s_ids in phrases_dict.values()])\n",
    "        for phrase_tuple, s_ids in phrases_dict.items():\n",
    "            phrase_len = len(phrase_tuple)\n",
    "            phrase_sents_count = len(s_ids)\n",
    "            \n",
    "            if not stems_phrases_count[stem].get(phrase_len):\n",
    "                stems_phrases_count[stem][phrase_len] = defaultdict(int)\n",
    "                \n",
    "            # count number of such phrase type for given stem\n",
    "            stems_phrases_count[stem][phrase_len][phrase_type] += phrase_sents_count \n",
    "\n",
    "            # count total number of such phrase type\n",
    "            phrases_count[phrase_len][phrase_type] = phrases_count[phrase_len].get(phrase_type, 0) + phrase_sents_count\n",
    "\n",
    "            # count total number of all phrases\n",
    "            phrases_lengths_count[phrase_len] += phrase_sents_count\n",
    "            phrases_types_count[phrase_type] += phrase_sents_count\n",
    "\n",
    "all_phrases_count = sum(phrases_types_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {2: {'ADJP': 288142,\n",
       "              'ADVP': 418156,\n",
       "              'NP': 1985844,\n",
       "              'PP': 3328,\n",
       "              'VP': 66991},\n",
       "             3: {'ADJP': 44770,\n",
       "              'ADVP': 31064,\n",
       "              'NP': 1353053,\n",
       "              'PP': 7554,\n",
       "              'VP': 8215},\n",
       "             4: {'ADJP': 4179, 'ADVP': 223, 'NP': 429226},\n",
       "             5: {'ADJP': 245, 'NP': 53025}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: defaultdict(int, {'ADJP': 12, 'ADVP': 8, 'NP': 28, 'VP': 9}),\n",
       " 3: defaultdict(int, {'ADJP': 1, 'NP': 12, 'VP': 1}),\n",
       " 4: defaultdict(int, {'NP': 4})}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = 'strive'\n",
    "\n",
    "stems_phrases_count[stemmer.stem(WORD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NP', 2), -12.029594340781607),\n",
       " (('ADJP', 2), -12.87689220116881),\n",
       " (('NP', 3), -12.876892201168811),\n",
       " (('VP', 2), -13.164574273620593),\n",
       " (('ADVP', 2), -13.282357309276973),\n",
       " (('NP', 4), -13.97550448983692),\n",
       " (('VP', 3), -15.361798850956813),\n",
       " (('ADJP', 3), -15.361798850956813)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrases_model(word, stemmed=True):\n",
    "    \"\"\"\n",
    "        Return sorted log probs of phrase types and length for the word \n",
    "    \"\"\"\n",
    "    if not stemmed:\n",
    "        stem = stemmer.stem(word)\n",
    "    else:\n",
    "        stem = word\n",
    "    \n",
    "    if not stems_phrases_count.get(stem):\n",
    "        return []\n",
    "    \n",
    "    stem_phrases = stems_phrases_count[stem]\n",
    "    \n",
    "    phrases_probs = []\n",
    "    \n",
    "    for p_length, p_types_count in stem_phrases.items():\n",
    "        for p_type, p_type_count in p_types_count.items():\n",
    "            prob_len = phrases_lengths_count[p_length] / all_phrases_count\n",
    "            \n",
    "            prob_len_type = phrases_count[p_length][p_type] / phrases_lengths_count[p_length]\n",
    "            \n",
    "            prob_word_len_type = p_type_count / phrases_count[p_length][p_type]\n",
    "            \n",
    "            log_prob = sum(np.log(p) for p in [prob_word_len_type, prob_len_type, prob_len])\n",
    "            \n",
    "            phrases_probs.append(((p_type, p_length), log_prob))\n",
    "    \n",
    "    phrases, scores = [], []\n",
    "    for phr, s in sorted(phrases_probs, key=itemgetter(1), reverse=True):\n",
    "        phrases.append(phr)\n",
    "        scores.append(s)\n",
    "    \n",
    "#     scores = np.exp(scores)\n",
    "#     scores /= np.max(scores)\n",
    "    \n",
    "    return list(zip(phrases, scores))\n",
    "\n",
    "phrases_model(WORD, stemmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring phrases (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.25\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 5\n",
    "\n",
    "P_TYPE_SCORE = dict(phrases_model(WORD, stemmed=False))\n",
    "\n",
    "phrases_ = get_phrases(WORD, stemmed=False)\n",
    "\n",
    "all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "for p_type, phrases in phrases_.items():\n",
    "    phrases_ = list(phrases)\n",
    "    \n",
    "    scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                   for phr in phrases_]))\n",
    "\n",
    "    for phr, sc in zip(phrases_, scores):\n",
    "        if not all_scored_phrases[p_type].get(len(phr)):\n",
    "            all_scored_phrases[p_type][len(phr)] = []\n",
    "        \n",
    "        score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "        \n",
    "        all_scored_phrases[p_type][len(phr)].append((phr, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVP\n",
      "\n",
      "2\n",
      "[('strive against', 47.570972129232146),\n",
      " ('strive with', 47.65035311830685),\n",
      " ('strive for', 56.87991182459042),\n",
      " ('striving after', 60.76540462625668),\n",
      " ('strive at', 72.63408748758481)]\n",
      "\n",
      "VP\n",
      "\n",
      "2\n",
      "[('striving against', 47.36485181683348),\n",
      " ('striving with', 47.444232805908186),\n",
      " ('striving for', 56.673791512191755),\n",
      " ('strive not', 57.704012020767195),\n",
      " ('striving in', 58.118454083145124)]\n",
      "\n",
      "3\n",
      "[('strive or fly', 53.73731564514743)]\n",
      "\n",
      "NP\n",
      "\n",
      "2\n",
      "[('strive against', 45.37863693436525),\n",
      " ('also strive', 45.6956552048578),\n",
      " ('thou strive', 51.02504954537294),\n",
      " ('economic strivings', 51.26184067925355),\n",
      " ('the strivings', 52.57374654015443)]\n",
      "\n",
      "3\n",
      "[('the time striving', 42.21950544921492),\n",
      " ('no man strive', 42.35136284438704),\n",
      " ('dost thou strive', 44.11854281035994),\n",
      " ('a striving after', 44.736132760921464),\n",
      " ('a critic striving', 47.06441893188094)]\n",
      "\n",
      "4\n",
      "[('social and economic strivings', 26.75404972739356),\n",
      " ('than a critic striving', 32.95443148322242),\n",
      " ('the potsherd strive with', 34.64571375556129),\n",
      " ('one powerful nation strives', 36.17675871558326)]\n",
      "\n",
      "ADJP\n",
      "\n",
      "2\n",
      "[('also strive', 47.178426460535405),\n",
      " ('so striving', 51.986163334162114),\n",
      " ('so strive', 51.986163334162114),\n",
      " ('vainly striving', 56.30455656460035),\n",
      " ('not striving', 57.20056839397657)]\n",
      "\n",
      "3\n",
      "[('not always strive', 52.29377697937595)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    print('%s\\n' % p_type)\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "        print(n)\n",
    "        pprint(best_phr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ADJP': [('also strive', 47.178426460535405),\n",
       "              ('so striving', 51.986163334162114),\n",
       "              ('so strive', 51.986163334162114),\n",
       "              ('not always strive', 52.29377697937595),\n",
       "              ('vainly striving', 56.30455656460035)],\n",
       "             'ADVP': [('strive against', 47.570972129232146),\n",
       "              ('strive with', 47.65035311830685),\n",
       "              ('strive for', 56.87991182459042),\n",
       "              ('striving after', 60.76540462625668),\n",
       "              ('strive at', 72.63408748758481)],\n",
       "             'NP': [('social and economic strivings', 26.75404972739356),\n",
       "              ('than a critic striving', 32.95443148322242),\n",
       "              ('the potsherd strive with', 34.64571375556129),\n",
       "              ('one powerful nation strives', 36.17675871558326),\n",
       "              ('the time striving', 42.21950544921492)],\n",
       "             'VP': [('striving against', 47.36485181683348),\n",
       "              ('striving with', 47.444232805908186),\n",
       "              ('strive or fly', 53.73731564514743),\n",
       "              ('striving for', 56.673791512191755),\n",
       "              ('strive not', 57.704012020767195)]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group different length together\n",
    "\n",
    "candidate_phrases = defaultdict(list)\n",
    "\n",
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "        \n",
    "        candidate_phrases[p_type].extend(best_phr)\n",
    "        \n",
    "    # TODO: filter similar phrases and phrases with keywords\n",
    "    candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "    \n",
    "candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of scoring phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add probabilistic CFG\n",
    "\n",
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.5\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 10\n",
    "\n",
    "def get_scored_phrases(word, include_scores=True):\n",
    "    P_TYPE_SCORE = dict(phrases_model(word, stemmed=False))\n",
    "\n",
    "    phrases_ = get_phrases(word, stemmed=False)\n",
    "\n",
    "    all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "    for p_type, phrases in phrases_.items():\n",
    "        phrases_ = list(phrases)\n",
    "\n",
    "        scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                       for phr in phrases_]))\n",
    "\n",
    "        for phr, sc in zip(phrases_, scores):\n",
    "            if not all_scored_phrases[p_type].get(len(phr)):\n",
    "                all_scored_phrases[p_type][len(phr)] = []\n",
    "\n",
    "            score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "\n",
    "            all_scored_phrases[p_type][len(phr)].append((phr, score))\n",
    "\n",
    "    # group different length together\n",
    "\n",
    "    candidate_phrases = defaultdict(list)\n",
    "\n",
    "    for p_type, scored_phrases in all_scored_phrases.items():\n",
    "        for n in scored_phrases:\n",
    "            nps = scored_phrases[n]\n",
    "            best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "            best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "            candidate_phrases[p_type].extend(best_phr)\n",
    "\n",
    "        # TODO: filter similar phrases and phrases with keywords\n",
    "        candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)#[:TOP_N]\n",
    "        \n",
    "        if not include_scores:\n",
    "            candidate_phrases[p_type] = [p for p, _ in candidate_phrases[p_type]]\n",
    "            \n",
    "    return candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  874\n"
     ]
    }
   ],
   "source": [
    "kws = get_test_keywords('data/lingualeo_words.csv')\n",
    "\n",
    "print('Keywords: ', len(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test keywords: ['it', 'far', 'tremendous', 'frown', 'prosecution']\n",
      "Test keywords stems: ['it', 'far', 'tremend', 'frown', 'prosecut']\n",
      "All kws found: False\n",
      "\n",
      "Test keywords: ['it', 'feed', 'income', 'astride', 'precedence']\n",
      "Test keywords stems: ['it', 'feed', 'incom', 'astrid', 'preced']\n",
      "All kws found: False\n",
      "\n",
      "Test keywords: ['percolate', 'kin', 'firm', 'cut', 'equally']\n",
      "Test keywords stems: ['percol', 'kin', 'firm', 'cut', 'equal']\n",
      "All kws found: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5\n",
    "np.random.seed = 0\n",
    "\n",
    "all_kws_found = False\n",
    "\n",
    "while not all_kws_found:    \n",
    "    test_kws = list(np.random.choice(list(kws), size=TEST_SIZE, replace=False))\n",
    "\n",
    "    word_kws = list(test_kws)\n",
    "    \n",
    "    print('Test keywords:', word_kws)\n",
    "    \n",
    "    test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "    kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "    print('Test keywords stems:', test_kws)\n",
    "\n",
    "    all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    print('All kws found:', all_kws_found)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('percol', 'kin'), ('firm',), ('cut',), ('equal',)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cluster_keywords(kws):\n",
    "    pairs = []\n",
    "\n",
    "    for i, kw in enumerate(kws):\n",
    "        cp = list(kws)\n",
    "        cp.remove(kw)\n",
    "\n",
    "        dsts = w2v.distances(kw, cp)\n",
    "#         print(kw, dsts)\n",
    "        max_ind = np.argmin(dsts)\n",
    "#         print(i, max_ind)\n",
    "        sim_kw = kws[max_ind + (1 if i <= max_ind else 0)]\n",
    "#         print(kw, sim_kw)\n",
    "\n",
    "        pair = (kw, sim_kw)\n",
    "\n",
    "        if (sim_kw, kw) in pairs:\n",
    "            pair = (sim_kw, kw)\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "    final_clusters = []\n",
    "    \n",
    "    kws_ = list(kws)\n",
    "    \n",
    "    for kws_pair, cnt in Counter(pairs).items():\n",
    "        if cnt > 1:\n",
    "            final_clusters.append(kws_pair)\n",
    "            [kws_.remove(w) for w in kws_pair]\n",
    "        \n",
    "    return final_clusters + [(kw,) for kw in kws_]\n",
    "\n",
    "cluster_keywords(test_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: 194 out of 197\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'job_no' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, sentences, total_sentences, chunksize, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                 \u001b[0mjob_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob_no\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-5f4ee3c9b04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_sents_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkws_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_scored_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkws_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranked_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkws_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-45a8d41c4def>\u001b[0m in \u001b[0;36mrank_sents\u001b[0;34m(ss, kwss, top_n)\u001b[0m\n\u001b[1;32m     10\u001b[0m                   for _, s in ss]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_stems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, sentences, total_sentences, chunksize, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mjob_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached end of input; waiting to finish %i outstanding jobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_no\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone_jobs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mjob_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# give the workers heads up that they can finish -- no more work!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'job_no' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for kws_tuple in cluster_keywords(test_kws):\n",
    "    result = gen_sents_candidates(kws_tuple, [get_scored_phrases(kw, include_scores=False) for kw in kws_tuple])\n",
    "    \n",
    "    ranked_sents = rank_sents(result, test_kws)\n",
    "    \n",
    "    print(kws_tuple, len(ranked_sents))\n",
    "    pprint(ranked_sents)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_rules = Cache.load_terminal_rules_sents()\n",
    "\n",
    "total_sents = sum(v['S'] for v in sents_rules.values())\n",
    "\n",
    "def rank_sents(ss, kwss, top_n=20):\n",
    "    res_sents = []\n",
    "    \n",
    "    sents_stems = [[stems_map.get(vocab_ind.get(w, w), w) \n",
    "                    for w in s.split(' ')]\n",
    "                  for _, s in ss]\n",
    "    \n",
    "    scores = model.score(sents_stems)\n",
    "    \n",
    "    for (s_prob, s), score in zip(ss, scores):\n",
    "        s_spl = s.split(' ')\n",
    "\n",
    "        if len(s_spl) > 2:\n",
    "            not_sw_prob = len([w\n",
    "                           for w in s_spl\n",
    "                           if w.lower() not in stop_words and stems_map[vocab_ind[w.lower()]] not in kwss]) / len(s_spl)\n",
    "            \n",
    "            print(score, np.log(s_prob or 1e-6), np.log(not_sw_prob or 1e-6))\n",
    "            \n",
    "            res_sents.append((s.capitalize() + '.', score + np.log(s_prob or 1e-6) + np.log(not_sw_prob or 1e-6)))\n",
    "            \n",
    "    return sorted(res_sents, key=itemgetter(1), reverse=True)[:top_n]\n",
    "    \n",
    "def combine_elements(*args):\n",
    "    return list(product(*args, repeat=1))\n",
    "\n",
    "def gen_sents_candidates(kws, kws_phrases):\n",
    "    marks = {'COMMA': [\",\"],\n",
    "             'COLON': [\":\"],\n",
    "             'SEMICOLON': [\";\"],\n",
    "             # 'DOT': [\".\"],\n",
    "             'QUESTION': [\"?\"],\n",
    "             'EXCLAM': [\"!\"],\n",
    "             'DASH': [\"-\"]}\n",
    "    \n",
    "    res_sents = set()\n",
    "\n",
    "    if len(kws) == 1:\n",
    "        kw = kws[0]\n",
    "        phrs = kws_phrases[0]\n",
    "        \n",
    "        for p_type, p_type_sents in stems_phrases[kw].items():\n",
    "            res_sents.update((0, ' '.join(sents[s_id])) for s_ids_set in p_type_sents.values() for s_id in list(s_ids_set) )\n",
    "            \n",
    "        return res_sents\n",
    "    \n",
    "    phrases = defaultdict(dict)\n",
    "\n",
    "    for kw, kw_phrases in zip(kws, kws_phrases):\n",
    "        for p_type in kw_phrases:\n",
    "            phrases[p_type][kw] = kw_phrases[p_type]\n",
    "\n",
    "    skipped = 0\n",
    "    \n",
    "    for sents_rule, sents_count in sents_rules.items():\n",
    "        s_prob = max(sents_count['S'], 1000) / total_sents\n",
    "        \n",
    "        if s_prob < 0.05:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        sent_symbols = str2ngram(sents_rule)\n",
    "        \n",
    "        sent_phrases = []\n",
    "        \n",
    "        # form lists of phrases of each keyword\n",
    "        for p_type in sent_symbols:\n",
    "            if p_type not in marks:\n",
    "                sent_phrases.append([(kw, phrases[p_type].get(kw))\n",
    "                                     for kw in kws\n",
    "                                    if phrases[p_type].get(kw)])\n",
    "            else:\n",
    "                sent_phrases.append((None, marks.get(p_type, [])))\n",
    "            \n",
    "        # combine phrases of keywords\n",
    "        kws_phrases_product = combine_elements(*sent_phrases)\n",
    "        \n",
    "        # combine sents\n",
    "        for kws_comb in kws_phrases_product:\n",
    "            if not kws_comb is None:\n",
    "                continue\n",
    "        \n",
    "            un_combs = [kw for kw, _ in kws_comb \n",
    "                        if kw is not None]\n",
    "            if len(set(un_combs)) <= 1:\n",
    "                continue\n",
    "            \n",
    "            kws_comb = [comb for _, comb in kws_comb]\n",
    "            \n",
    "            sents_candidates = [(s_prob, ' '.join(s))\n",
    "                                for s in combine_elements(*kws_comb)]\n",
    "            \n",
    "            res_sents.update([s_cand for s_cand in sents_candidates])\n",
    "    \n",
    "    print('Skipped: %s out of %s' % (skipped, len(sents_rules)))\n",
    "    return res_sents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
