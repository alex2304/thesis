{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "\n",
    "from utils.nltk_utils import *\n",
    "from utils.ngrams import *\n",
    "from utils.eval import get_test_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.grammar_utils import tags_seq_to_symbols\n",
    "from utils.io import Cache\n",
    "from utils.ngrams import ngram2str\n",
    "\n",
    "\n",
    "observed_tags = Cache.load_observed_tags()\n",
    "terminal_rules = Cache.load_terminal_rules()\n",
    "\n",
    "\n",
    "def parse_phrases(tt_ngrams) -> Tuple[List, List[List[Tuple]]]:\n",
    "    global observed_tags, terminal_rules\n",
    "    \n",
    "    phrases = []\n",
    "    phrases_types = []\n",
    "\n",
    "    if observed_tags is None:\n",
    "        observed_tags = dict()\n",
    "\n",
    "    for tt_gram in tt_ngrams:\n",
    "        symbols = tuple(tags_seq_to_symbols([tag\n",
    "                                             for _, tag in tt_gram]))\n",
    "\n",
    "        phrase = tt_gram\n",
    "\n",
    "        # check if tags phrase has been already observed\n",
    "        tags_str = ngram2str(symbols)\n",
    "\n",
    "        if tags_str in observed_tags:\n",
    "            if observed_tags[tags_str] is not None:\n",
    "                phrases.append(phrase)\n",
    "                phrases_types.append(observed_tags[tags_str])\n",
    "\n",
    "            continue\n",
    "\n",
    "        p_types_dict = terminal_rules.get(tags_str)\n",
    "\n",
    "        if p_types_dict:\n",
    "            p_type = max(p_types_dict, key=lambda k: p_types_dict[k])\n",
    "\n",
    "            phrases.append(phrase)\n",
    "            phrases_types.append(p_type)\n",
    "\n",
    "            observed_tags[tags_str] = p_type\n",
    "\n",
    "        else:\n",
    "            observed_tags[tags_str] = None\n",
    "\n",
    "#     try:\n",
    "#         Cache.save_observed_tags(observed_tags)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc(e)\n",
    "\n",
    "    return phrases_types, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/tk/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/tk/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /Users/tk/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "[nltk.download(corp_) for corp_ in ['gutenberg', 'brown', 'reuters']]\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "sents = gutenberg.sents() + brown.sents() + reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 210608\n",
      "Words: 5503894\n",
      "Vocab: 87046\n"
     ]
    }
   ],
   "source": [
    "sents = [tuple(s) \n",
    "         for s in sents]\n",
    "\n",
    "words = [w.lower() \n",
    "         for s in sents for w in s]\n",
    "\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "print('Sents:', len(sents))\n",
    "print('Words:', len(words))\n",
    "print('Vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indexes\n",
    "\n",
    "# {sentence: sent_index}\n",
    "sent2ind = {sents[i]: i for i in range(len(sents))}\n",
    "\n",
    "# {word: word_index}\n",
    "word2ind = {vocab[j]: j for j in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping from words to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: 58858\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in vocab]\n",
    "\n",
    "# {word_index: stem}\n",
    "stems_map = {word2ind[word]: stem \n",
    "             for word, stem in zip(vocab, stems)}\n",
    "\n",
    "print('Stems:', len(set(stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2word = {stem: vocab[ind]\n",
    "             for ind, stem in stems_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained and saved to data/cache/w2v_CBOW_300_15_hs_all\n",
      "[('esther', 0.4287969768047333), ('queen', 0.40083709359169006), ('daughter', 0.3828454911708832), ('princess', 0.3554416000843048), ('theatre-by-the-sea', 0.3367374837398529), ('sister', 0.33462944626808167), ('sarason', 0.33333131670951843), ('candac', 0.33151155710220337), ('jarmuth', 0.32717573642730713), ('hegai', 0.32047152519226074)]\n",
      "0.28550296254314844\n",
      "bring\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "VECTOR_SIZE = 300\n",
    "WINDOW = 15\n",
    "w2v_file_name = 'data/cache/w2v_CBOW_%d_%d_hs_all' % (VECTOR_SIZE, WINDOW)\n",
    "try:\n",
    "    model = gensim.models.Word2Vec.load(w2v_file_name)\n",
    "    print('Loaded from file %s' % w2v_file_name)\n",
    "except FileNotFoundError:\n",
    "    sents_stemmed = [[stems_map[word2ind[w.lower()]] \n",
    "                      for w in s] \n",
    "                     for s in sents]\n",
    "    model = gensim.models.Word2Vec(sentences=sents_stemmed, size=300, window=15, min_count=1, hs=1, negative=0)\n",
    "    model.save(w2v_file_name)\n",
    "    print('Trained and saved to %s' % w2v_file_name)\n",
    "\n",
    "# testing\n",
    "print(model.wv.most_similar(positive=[stemmer.stem('woman'), stemmer.stem('king')], negative=[stemmer.stem('man')]))\n",
    "print(model.wv.similarity(stemmer.stem('campus'), stemmer.stem('dormitory')))\n",
    "print(model.wv.doesnt_match([stemmer.stem(w) for w in \"dormitory bring campus\".split()]))\n",
    "\n",
    "w2v = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210608it [07:23, 475.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NP': defaultdict(set,\n",
       "             {(8628, 77456): {187,\n",
       "               66413,\n",
       "               78058,\n",
       "               81366,\n",
       "               115127,\n",
       "               120880,\n",
       "               125006,\n",
       "               131072,\n",
       "               133693,\n",
       "               133696,\n",
       "               144948,\n",
       "               157951,\n",
       "               159366,\n",
       "               178610,\n",
       "               182963,\n",
       "               197604,\n",
       "               200077,\n",
       "               203270,\n",
       "               203391},\n",
       "              (8628, 4142): {10854,\n",
       "               90541,\n",
       "               91285,\n",
       "               97039,\n",
       "               102810,\n",
       "               124984,\n",
       "               128804,\n",
       "               129251,\n",
       "               146735,\n",
       "               192731,\n",
       "               210054},\n",
       "              (6841, 8629): {67418, 129544, 141032},\n",
       "              (8629, 5870): {67418},\n",
       "              (6841, 8629, 5870): {67418},\n",
       "              (8629, 5870, 54834): {67418},\n",
       "              (85626, 6841, 8629, 5870): {67418},\n",
       "              (6841, 8629, 5870, 54834): {67418},\n",
       "              (8628, 78866): {70195, 74759},\n",
       "              (53251, 8628): {78058},\n",
       "              (77456, 62432, 8629): {79529},\n",
       "              (49851, 8628): {81366},\n",
       "              (51453, 8631): {82960},\n",
       "              (8631, 35292): {82960},\n",
       "              (28852, 51453, 8631): {82960},\n",
       "              (51453, 8631, 35292): {82960},\n",
       "              (28852, 51453, 8631, 35292): {82960},\n",
       "              (77432, 12992, 8629): {85266},\n",
       "              (69369, 8628): {85348},\n",
       "              (8628, 77604): {85527},\n",
       "              (8628, 48292): {85693},\n",
       "              (71047, 8628): {86143},\n",
       "              (8631, 9098): {87054},\n",
       "              (8630, 72194): {89778},\n",
       "              (8630, 72194, 55785): {89778},\n",
       "              (8630, 72194, 55785, 38739, 32130): {89778},\n",
       "              (54085, 8628): {97073, 117626, 192731},\n",
       "              (8628, 77432): {97073,\n",
       "               104759,\n",
       "               105355,\n",
       "               116435,\n",
       "               120802,\n",
       "               121453,\n",
       "               124985,\n",
       "               125318,\n",
       "               128875,\n",
       "               129190,\n",
       "               129706,\n",
       "               131994,\n",
       "               132229,\n",
       "               132819,\n",
       "               132875,\n",
       "               134690,\n",
       "               135168,\n",
       "               142539,\n",
       "               150797,\n",
       "               163081,\n",
       "               181475},\n",
       "              (68158, 8630): {97769},\n",
       "              (62471, 8628): {102810},\n",
       "              (65947, 8631): {108029},\n",
       "              (8631, 7074, 19629): {112714},\n",
       "              (8628, 7409): {117845, 188476},\n",
       "              (77791, 8629): {118657},\n",
       "              (8628, 54885): {123829},\n",
       "              (30153, 8630): {124887},\n",
       "              (8631, 29071, 19830): {125048},\n",
       "              (8629, 65152): {127306},\n",
       "              (73393, 8631): {128765},\n",
       "              (8631, 65152): {128765},\n",
       "              (53261, 73393, 8631): {128765},\n",
       "              (73393, 8631, 65152): {128765},\n",
       "              (77456, 53261, 73393, 8631): {128765},\n",
       "              (8631, 77571, 28255): {129118},\n",
       "              (8631, 5414): {129133, 136426},\n",
       "              (77432, 8629): {129262},\n",
       "              (8629, 40042): {129544},\n",
       "              (6841, 8629, 40042): {129544},\n",
       "              (8629, 40042, 82795): {129544},\n",
       "              (6958, 6841, 8629, 40042): {129544},\n",
       "              (6841, 8629, 40042, 82795): {129544},\n",
       "              (8631, 72837): {129581},\n",
       "              (8629, 9098): {131420},\n",
       "              (4142, 79104, 8629): {133063},\n",
       "              (8628, 53631): {133114},\n",
       "              (8628, 43651): {133304},\n",
       "              (8629, 19116): {136027},\n",
       "              (16046, 8629, 19116): {136027},\n",
       "              (78030, 8628): {136474},\n",
       "              (8628, 6841): {136474},\n",
       "              (77456, 82556, 8629): {136936},\n",
       "              (8629, 52570): {141032},\n",
       "              (6841, 8629, 52570): {141032},\n",
       "              (80827, 6841, 8629, 52570): {141032},\n",
       "              (8629, 2335): {158434},\n",
       "              (8631, 40194): {163802},\n",
       "              (8628, 72194): {166865},\n",
       "              (8628, 4425): {167315},\n",
       "              (8631, 15479): {178064},\n",
       "              (8631, 15479, 41682): {178064},\n",
       "              (8628, 24096): {179547},\n",
       "              (8629, 20598): {179758},\n",
       "              (8631, 54955): {180364, 203646, 209266},\n",
       "              (8631, 54955, 60864): {180364, 203646, 209266},\n",
       "              (8631, 21209, 54955): {180752},\n",
       "              (8631, 21209): {180864},\n",
       "              (54194, 8628): {181475},\n",
       "              (8629, 19830): {186527},\n",
       "              (8628, 6620): {188470, 188472},\n",
       "              (6425, 8628): {188476},\n",
       "              (8631, 6620): {188480},\n",
       "              (8631, 20059): {191693},\n",
       "              (8631, 20059, 60864): {191693},\n",
       "              (8629, 36199): {193280},\n",
       "              (53178, 8630): {195818},\n",
       "              (1677, 53178, 8630): {195818},\n",
       "              (77456, 41451, 8629): {204205},\n",
       "              (8631, 48669): {204740},\n",
       "              (8631, 48669, 19116): {204740},\n",
       "              (8631, 19905): {205379},\n",
       "              (8631, 29135): {206542}}),\n",
       " 'ADJP': defaultdict(set,\n",
       "             {(5566, 8631): {53379},\n",
       "              (75026, 8629): {62560},\n",
       "              (28806, 8631): {78913},\n",
       "              (8629, 8322): {100786, 136405, 136436},\n",
       "              (8629, 77432): {103574,\n",
       "               129208,\n",
       "               129583,\n",
       "               129699,\n",
       "               130147,\n",
       "               130238,\n",
       "               130557,\n",
       "               131722,\n",
       "               133059,\n",
       "               133415,\n",
       "               134962,\n",
       "               136578,\n",
       "               143528,\n",
       "               155524,\n",
       "               164946},\n",
       "              (8629, 40323): {107429, 118657, 129508, 132871, 133063, 133149},\n",
       "              (10042, 8629): {122610},\n",
       "              (8629, 31882): {128992},\n",
       "              (8629, 14426): {129262, 163315, 166916, 188102},\n",
       "              (54928, 8631): {131253},\n",
       "              (79104, 8629): {133063},\n",
       "              (76214, 8629): {134962},\n",
       "              (16046, 8629): {136027},\n",
       "              (8629, 10767): {142473},\n",
       "              (6509, 8629): {155524},\n",
       "              (22386, 8629): {163315, 166916},\n",
       "              (54085, 8631): {188105}}),\n",
       " 'VP': defaultdict(set,\n",
       "             {(8631, 31882): {79580},\n",
       "              (8631, 77432): {108029, 129016, 132751, 177963},\n",
       "              (8629, 6352): {153680},\n",
       "              (8628, 361, 47368): {158610}}),\n",
       " 'ADVP': defaultdict(set,\n",
       "             {(77542, 8630): {89778},\n",
       "              (70362, 8628): {191787},\n",
       "              (8628, 77456): {191787},\n",
       "              (77736, 8630): {195255}})}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'data/cache/stems_phrases'\n",
    "try:\n",
    "    with open(file_name, mode='rb') as fp:\n",
    "        stems_phrases = pickle.load(fp)\n",
    "    print('Loade from file: %s' % file_name)\n",
    "except FileNotFoundError:\n",
    "    parser = NGramsParser()\n",
    "    sents_words_indexes = parser.parse_sents_tokens(sents)\n",
    "\n",
    "    # {stem: Set[sentence_ind]}\n",
    "    stems_phrases = defaultdict(dict)\n",
    "\n",
    "    for i, (s_words, words_indexes) in tqdm(enumerate(zip(sents, sents_words_indexes), start=1)):\n",
    "        words_ttokens = nltk.pos_tag([w.lower() for w in s_words])\n",
    "\n",
    "        tt_ngrams = [ngr\n",
    "                     for i in range(2, 5 + 1) \n",
    "                     for ngr in n_grams(words_ttokens, i, words_indexes, pad_left=False)]\n",
    "\n",
    "        types, phrases = parse_phrases(tt_ngrams)\n",
    "\n",
    "        # format and store phrases\n",
    "        for t, p in zip(types, phrases):\n",
    "            phrase_inds = tuple(word2ind[token] for token, _ in p)\n",
    "\n",
    "            for word_ind in phrase_inds:\n",
    "                if vocab[word_ind] not in stop_words:\n",
    "                    stem = stems_map[word_ind]\n",
    "\n",
    "                    if not stems_phrases[stem].get(t):\n",
    "                        stems_phrases[stem][t] = defaultdict(set)\n",
    "\n",
    "                    stem_phr_t = stems_phrases[stem][t]\n",
    "\n",
    "                    stem_phr_t[phrase_inds].add(sent2ind[s_words])\n",
    "\n",
    "    # save\n",
    "    with open(file_name, mode='wb') as fp:\n",
    "        pickle.dump(stems_phrases, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(word, p_type='NP'):\n",
    "    obj = stems_phrases[stemmer.stem(word)]\n",
    "\n",
    "    for phrase, sss in obj[p_type].items():\n",
    "        p = ' '.join([vocab[ind] for ind in phrase])\n",
    "        yield p, [sents[ind] for ind in sss]\n",
    "\n",
    "g = explore('assume', 'NP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('that bowsman assumed',\n",
       " [('It',\n",
       "   'so',\n",
       "   'chanced',\n",
       "   ',',\n",
       "   'that',\n",
       "   'after',\n",
       "   'the',\n",
       "   'Parsee',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'disappearance',\n",
       "   ',',\n",
       "   'I',\n",
       "   'was',\n",
       "   'he',\n",
       "   'whom',\n",
       "   'the',\n",
       "   'Fates',\n",
       "   'ordained',\n",
       "   'to',\n",
       "   'take',\n",
       "   'the',\n",
       "   'place',\n",
       "   'of',\n",
       "   'Ahab',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'bowsman',\n",
       "   ',',\n",
       "   'when',\n",
       "   'that',\n",
       "   'bowsman',\n",
       "   'assumed',\n",
       "   'the',\n",
       "   'vacant',\n",
       "   'post',\n",
       "   ';',\n",
       "   'the',\n",
       "   'same',\n",
       "   ',',\n",
       "   'who',\n",
       "   ',',\n",
       "   'when',\n",
       "   'on',\n",
       "   'the',\n",
       "   'last',\n",
       "   'day',\n",
       "   'the',\n",
       "   'three',\n",
       "   'men',\n",
       "   'were',\n",
       "   'tossed',\n",
       "   'from',\n",
       "   'out',\n",
       "   'of',\n",
       "   'the',\n",
       "   'rocking',\n",
       "   'boat',\n",
       "   ',',\n",
       "   'was',\n",
       "   'dropped',\n",
       "   'astern',\n",
       "   '.')])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for analysis\n",
    "\n",
    "phrases_count = defaultdict(dict)\n",
    "phrases_lengths_count = defaultdict(int)\n",
    "phrases_types_count = defaultdict(int)\n",
    "stems_phrases_count = defaultdict(dict)\n",
    "\n",
    "for stem, stem_phrases_dict in stems_phrases.items():\n",
    "    for phrase_type, phrases_dict in stem_phrases_dict.items():\n",
    "#         stem_phrase_type_count = sum([len(s_ids) for s_ids in phrases_dict.values()])\n",
    "        for phrase_tuple, s_ids in phrases_dict.items():\n",
    "            phrase_len = len(phrase_tuple)\n",
    "            phrase_sents_count = len(s_ids)\n",
    "            \n",
    "            if not stems_phrases_count[stem].get(phrase_len):\n",
    "                stems_phrases_count[stem][phrase_len] = defaultdict(int)\n",
    "                \n",
    "            # count number of such phrase type for given stem\n",
    "            stems_phrases_count[stem][phrase_len][phrase_type] += phrase_sents_count \n",
    "\n",
    "            # count total number of such phrase type\n",
    "            phrases_count[phrase_len][phrase_type] = phrases_count[phrase_len].get(phrase_type, 0) + phrase_sents_count\n",
    "\n",
    "            # count total number of all phrases\n",
    "            phrases_lengths_count[phrase_len] += phrase_sents_count\n",
    "            phrases_types_count[phrase_type] += phrase_sents_count\n",
    "\n",
    "all_phrases_count = sum(phrases_types_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4694125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: defaultdict(int, {'NP': 28, 'ADJP': 12, 'ADVP': 8, 'VP': 9}),\n",
       " 3: defaultdict(int, {'NP': 12, 'ADJP': 1, 'VP': 1}),\n",
       " 4: defaultdict(int, {'NP': 4})}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = 'strive'\n",
    "\n",
    "stems_phrases_count[stemmer.stem(WORD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NP', 2), -12.02961777460343),\n",
       " (('NP', 3), -12.876915634990633),\n",
       " (('ADJP', 2), -12.876915634990635),\n",
       " (('VP', 2), -13.164597707442416),\n",
       " (('ADVP', 2), -13.2823807430988),\n",
       " (('NP', 4), -13.975527923658742),\n",
       " (('ADJP', 3), -15.361822284778633),\n",
       " (('VP', 3), -15.361822284778635)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrases_model(word, stemmed=True):\n",
    "    \"\"\"\n",
    "        Return sorted log probs of phrase types and length for the word \n",
    "    \"\"\"\n",
    "    if not stemmed:\n",
    "        stem = stemmer.stem(word)\n",
    "    else:\n",
    "        stem = word\n",
    "    \n",
    "    if not stems_phrases_count.get(stem):\n",
    "        return []\n",
    "    \n",
    "    stem_phrases = stems_phrases_count[stem]\n",
    "    \n",
    "    phrases_probs = []\n",
    "    \n",
    "    for p_length, p_types_count in stem_phrases.items():\n",
    "        for p_type, p_type_count in p_types_count.items():\n",
    "            prob_len = phrases_lengths_count[p_length] / all_phrases_count \n",
    "            prob_len_type = phrases_count[p_length][p_type] / phrases_lengths_count[p_length]\n",
    "            prob_word_len_type = p_type_count / phrases_count[p_length][p_type]\n",
    "            \n",
    "            log_prob = sum(np.log(p) for p in [prob_word_len_type, prob_len_type, prob_len])\n",
    "            \n",
    "            phrases_probs.append(((p_type, p_length), log_prob))\n",
    "    \n",
    "    phrases, scores = [], []\n",
    "    for phr, s in sorted(phrases_probs, key=itemgetter(1), reverse=True):\n",
    "        phrases.append(phr)\n",
    "        scores.append(s)\n",
    "    \n",
    "#     scores = np.exp(scores)\n",
    "#     scores /= np.max(scores)\n",
    "    \n",
    "    return list(zip(phrases, scores))\n",
    "\n",
    "phrases_model(WORD, stemmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring phrases (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.25\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 5\n",
    "\n",
    "P_TYPE_SCORE = dict(phrases_model(WORD, stemmed=False))\n",
    "\n",
    "phrases_ = get_phrases(WORD, stemmed=False)\n",
    "\n",
    "all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "for p_type, phrases in phrases_.items():\n",
    "    phrases_ = list(phrases)\n",
    "    \n",
    "    scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                   for phr in phrases_]))\n",
    "\n",
    "    for phr, sc in zip(phrases_, scores):\n",
    "        if not all_scored_phrases[p_type].get(len(phr)):\n",
    "            all_scored_phrases[p_type][len(phr)] = []\n",
    "        \n",
    "        score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "        \n",
    "        all_scored_phrases[p_type][len(phr)].append((phr, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    print('%s\\n' % p_type)\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "        print(n)\n",
    "        pprint(best_phr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group different length together\n",
    "\n",
    "candidate_phrases = defaultdict(list)\n",
    "\n",
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "        \n",
    "        candidate_phrases[p_type].extend(best_phr)\n",
    "        \n",
    "    # TODO: filter similar phrases and phrases with keywords\n",
    "    candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "    \n",
    "candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of scoring phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(word, stemmed=True):\n",
    "    if stemmed:\n",
    "        stem = word\n",
    "    else:\n",
    "        stem = stemmer.stem(word)\n",
    "        \n",
    "    phrs = stems_phrases[stem]\n",
    "    \n",
    "    if not phrs:\n",
    "        return {}\n",
    "    \n",
    "    phrases_ = defaultdict(set)\n",
    "    \n",
    "    for p_type, phr_dict in phrs.items():\n",
    "        for phrase, sents in phr_dict.items():\n",
    "            phrase_ = tuple(vocab[ind] for ind in phrase)\n",
    "            \n",
    "            phrases_[p_type].add(phrase_)\n",
    "            \n",
    "    return phrases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add probabilistic CFG\n",
    "\n",
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.5\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 10\n",
    "\n",
    "def get_scored_phrases(word, include_scores=True):\n",
    "    P_TYPE_SCORE = dict(phrases_model(word, stemmed=False))\n",
    "\n",
    "    phrases_ = get_phrases(word, stemmed=False)\n",
    "\n",
    "    all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "    for p_type, phrases in phrases_.items():\n",
    "        phrases_ = list(phrases)\n",
    "\n",
    "        scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                       for phr in phrases_]))\n",
    "\n",
    "        for phr, sc in zip(phrases_, scores):\n",
    "            if not all_scored_phrases[p_type].get(len(phr)):\n",
    "                all_scored_phrases[p_type][len(phr)] = []\n",
    "\n",
    "            score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "\n",
    "            all_scored_phrases[p_type][len(phr)].append((phr, score))\n",
    "\n",
    "    # group different length together\n",
    "\n",
    "    candidate_phrases = defaultdict(list)\n",
    "\n",
    "    for p_type, scored_phrases in all_scored_phrases.items():\n",
    "        for n in scored_phrases:\n",
    "            nps = scored_phrases[n]\n",
    "            best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "            best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "            candidate_phrases[p_type].extend(best_phr)\n",
    "\n",
    "        # TODO: filter similar phrases and phrases with keywords\n",
    "        candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)#[:TOP_N]\n",
    "        \n",
    "        if not include_scores:\n",
    "            candidate_phrases[p_type] = [p for p, _ in candidate_phrases[p_type]]\n",
    "            \n",
    "    return candidate_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems_phrases['assum']['ADJP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ' '.join(nltk.flatten(sents))\n",
    "\n",
    "escape_chars = ['æ', 'æ','è','é','î','ü', '\\n', '\\x1a']\n",
    "\n",
    "for c in escape_chars:\n",
    "    data = data.replace(c, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "SEQS_NUM = int(np.floor(len(data)/SEQ_LENGTH))\n",
    "\n",
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "print('SEQ_LENGTH =', SEQ_LENGTH, '\\nNumber of sequences:', SEQS_NUM, '\\nVOCAB_SIZE =', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 500\n",
    "LAYER_NUM = 3\n",
    "\n",
    "rnn = Sequential()\n",
    "\n",
    "rnn.add(layers.LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    rnn.add(layers.LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "\n",
    "rnn.add(layers.TimeDistributed(layers.Dense(VOCAB_SIZE)))\n",
    "rnn.add(layers.Activation('softmax'))\n",
    "\n",
    "rnn.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "rnn.load_weights('trained_large/checkpoint_500_all_epoch_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_from_context(mdl, context, length=100):\n",
    "    ix = [char_to_ix[c] for c in context]\n",
    "\n",
    "    text = [c for c in context]\n",
    "\n",
    "    X = np.zeros((1, length + len(context), VOCAB_SIZE))\n",
    "\n",
    "    for i, ind in enumerate(ix):\n",
    "        X[0, i, :][ind] = 1\n",
    "\n",
    "        print(ix_to_char[ind], end=\"\")\n",
    "\n",
    "    for i in range(len(context), length + len(context)):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "\n",
    "        ix = np.argmax(mdl.predict(X[:, :i+1, :])[0], 1)\n",
    "\n",
    "        text.append(ix_to_char[ix[-1]])\n",
    "\n",
    "    return ('').join(text)\n",
    "  \n",
    "generate_from_context(rnn, 'to recreational facilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kws = get_test_keywords('data/lingualeo_words.csv')\n",
    "\n",
    "print('Keywords: ', len(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_kws(cnt):\n",
    "    all_kws_found = False\n",
    "\n",
    "    while not all_kws_found:    \n",
    "        test_kws = list(np.random.choice(list(kws), size=cnt, replace=False))\n",
    "\n",
    "        word_kws = list(test_kws)\n",
    "\n",
    "        test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "        kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "\n",
    "        all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    return test_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 5\n",
    "np.random.seed = 0\n",
    "\n",
    "all_kws_found = False\n",
    "\n",
    "while not all_kws_found:    \n",
    "    test_kws = list(np.random.choice(list(kws), size=TEST_SIZE, replace=False))\n",
    "\n",
    "    word_kws = list(test_kws)\n",
    "    \n",
    "    print('Test keywords:', word_kws)\n",
    "    \n",
    "    test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "    kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "    print('Test keywords stems:', test_kws)\n",
    "\n",
    "    all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    print('All kws found:', all_kws_found)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_keywords(kws):\n",
    "    pairs = []\n",
    "\n",
    "    for i, kw in enumerate(kws):\n",
    "        cp = list(kws)\n",
    "        cp.remove(kw)\n",
    "\n",
    "        dsts = w2v.distances(kw, cp)\n",
    "#         print(kw, dsts)\n",
    "        max_ind = np.argmin(dsts)\n",
    "#         print(i, max_ind)\n",
    "        sim_kw = kws[max_ind + (1 if i <= max_ind else 0)]\n",
    "#         print(kw, sim_kw)\n",
    "\n",
    "        pair = (kw, sim_kw)\n",
    "\n",
    "        if (sim_kw, kw) in pairs:\n",
    "            pair = (sim_kw, kw)\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "    final_clusters = []\n",
    "    \n",
    "    kws_ = list(kws)\n",
    "    \n",
    "    for kws_pair, cnt in Counter(pairs).items():\n",
    "        if cnt > 1:\n",
    "            final_clusters.append(kws_pair)\n",
    "            [kws_.remove(w) for w in kws_pair]\n",
    "        \n",
    "    return final_clusters + [(kw,) for kw in kws_]\n",
    "\n",
    "cluster_keywords(test_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_rules = Cache.load_terminal_rules_sents()\n",
    "\n",
    "total_sents = sum(v['S'] for v in sents_rules.values())\n",
    "\n",
    "def rank_sents(ss, kwss, top_n=30):\n",
    "    res_sents = []\n",
    "    \n",
    "    sents_stems = [[stems_map.get(word2ind.get(w, w), w) \n",
    "                    for w in s.split(' ')]\n",
    "                  for _, s in ss]\n",
    "    \n",
    "    if not sents_stems:\n",
    "        return []\n",
    "    \n",
    "    scores = model.score(sents_stems)\n",
    "    \n",
    "    for (s_prob, s), score in zip(ss, scores):\n",
    "        s_spl = s.split(' ')\n",
    "\n",
    "        if len(s_spl) > 2:\n",
    "            not_sws = [w\n",
    "                       for w in s_spl\n",
    "                       if w.lower() not in stop_words and stems_map[word2ind[w.lower()]] not in kwss] \n",
    "#             print(not_sws)\n",
    "            not_sw_prob = len(not_sws) / len(s_spl) / 10\n",
    "            \n",
    "#             print(score, np.log(s_prob or 1e-6), np.log(not_sw_prob or 1e-6))\n",
    "            \n",
    "            res_sents.append((s, score + np.log(s_prob or 1e-8) + np.log(not_sw_prob or 1e-8)))\n",
    "            \n",
    "    return sorted(res_sents, key=itemgetter(1), reverse=True)[:top_n]\n",
    "    \n",
    "def combine_elements(*args):\n",
    "    return list(product(*args, repeat=1))\n",
    "\n",
    "def gen_sents_candidates(kws, kws_phrases):\n",
    "    marks = {'COMMA': [\",\"],\n",
    "             'COLON': [\":\"],\n",
    "             'SEMICOLON': [\";\"],\n",
    "             # 'DOT': [\".\"],\n",
    "             'QUESTION': [\"?\"],\n",
    "             'EXCLAM': [\"!\"],\n",
    "             'DASH': [\"-\"]}\n",
    "    \n",
    "    res_sents = set()\n",
    "\n",
    "    if len(kws) == 1:\n",
    "        kw = kws[0]\n",
    "        phrs = kws_phrases[0]\n",
    "        \n",
    "        for p_type, p_type_sents in stems_phrases[kw].items():\n",
    "            res_sents.update((0, ' '.join(sents[s_id])) for s_ids_set in p_type_sents.values() for s_id in list(s_ids_set) )\n",
    "            \n",
    "        return res_sents\n",
    "    \n",
    "    phrases = defaultdict(dict)\n",
    "\n",
    "    for kw, kw_phrases in zip(kws, kws_phrases):\n",
    "        for p_type in kw_phrases:\n",
    "            phrases[p_type][kw] = kw_phrases[p_type]\n",
    "\n",
    "    skipped = 0\n",
    "    \n",
    "    for sents_rule, sents_count in sents_rules.items():\n",
    "        s_prob = max(sents_count['S'], 1000) / total_sents\n",
    "        \n",
    "        if s_prob < 0.03:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        sent_symbols = str2ngram(sents_rule)\n",
    "        \n",
    "        sent_phrases = []\n",
    "        \n",
    "        # form lists of phrases of each keyword\n",
    "        for p_type in sent_symbols:\n",
    "            if p_type not in marks:\n",
    "                sent_phrases.append([(kw, phrases[p_type].get(kw))\n",
    "                                     for kw in kws\n",
    "                                    if phrases[p_type].get(kw)])\n",
    "            else:\n",
    "                sent_phrases.append((None, marks.get(p_type, [])))\n",
    "            \n",
    "        # combine phrases of keywords\n",
    "        kws_phrases_product = combine_elements(*sent_phrases)\n",
    "        \n",
    "        # combine sents\n",
    "        for kws_comb in kws_phrases_product:\n",
    "            if kws_comb is None:\n",
    "                continue\n",
    "        \n",
    "            un_combs = [kw \n",
    "                        for kw, _ in kws_comb \n",
    "                        if kw is not None]\n",
    "            \n",
    "            if len(set(un_combs)) <= 1:\n",
    "                continue\n",
    "            \n",
    "            kws_comb = [comb for _, comb in kws_comb]\n",
    "            \n",
    "            sents_candidates = [(s_prob, ' '.join(s))\n",
    "                                for s in combine_elements(*kws_comb)]\n",
    "            \n",
    "            res_sents.update([s_cand for s_cand in sents_candidates])\n",
    "    \n",
    "    print('Skipped: %s out of %s' % (skipped, len(sents_rules)))\n",
    "    return res_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_test(keywords):\n",
    "    best_sents = []\n",
    "    clusters = cluster_keywords(keywords)\n",
    "    \n",
    "    print('Clusters:', clusters)\n",
    "    \n",
    "    for kws_tuple in clusters:\n",
    "        result = gen_sents_candidates(kws_tuple, [get_scored_phrases(kw, include_scores=False) for kw in kws_tuple])\n",
    "\n",
    "        ranked_sents = rank_sents(result, keywords)\n",
    "        \n",
    "        best_sents.append([s for s, _ in ranked_sents[:3]])\n",
    "        \n",
    "    return ['. '.join([s.capitalize() for s in ss]) for ss in zip(*best_sents)]\n",
    "\n",
    "generate_test(test_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = '' or gen_random_kws(6)\n",
    "\n",
    "print('Keywords:', KEYWORDS)\n",
    "\n",
    "print('\\nCandidate text:')\n",
    "\n",
    "generate_test(KEYWORDS)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
