{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "\n",
    "from utils.nltk_utils import *\n",
    "from utils.ngrams import *\n",
    "from utils.eval import get_test_keywords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.grammar_utils import tags_seq_to_symbols\n",
    "from utils.io import Cache\n",
    "from utils.ngrams import ngram2str\n",
    "\n",
    "\n",
    "observed_tags = Cache.load_observed_tags()\n",
    "terminal_rules = Cache.load_terminal_rules()\n",
    "\n",
    "\n",
    "def parse_phrases(tt_ngrams) -> Tuple[List, List[List[Tuple]]]:\n",
    "    global observed_tags, terminal_rules\n",
    "    \n",
    "    phrases = []\n",
    "    phrases_types = []\n",
    "\n",
    "    if observed_tags is None:\n",
    "        observed_tags = dict()\n",
    "\n",
    "    for tt_gram in tt_ngrams:\n",
    "        symbols = tuple(tags_seq_to_symbols([tag\n",
    "                                             for _, tag in tt_gram]))\n",
    "\n",
    "        phrase = tt_gram\n",
    "\n",
    "        # check if tags phrase has been already observed\n",
    "        tags_str = ngram2str(symbols)\n",
    "\n",
    "        if tags_str in observed_tags:\n",
    "            if observed_tags[tags_str] is not None:\n",
    "                phrases.append(phrase)\n",
    "                phrases_types.append(observed_tags[tags_str])\n",
    "\n",
    "            continue\n",
    "\n",
    "        p_types_dict = terminal_rules.get(tags_str)\n",
    "\n",
    "        if p_types_dict:\n",
    "            p_type = max(p_types_dict, key=lambda k: p_types_dict[k])\n",
    "\n",
    "            phrases.append(phrase)\n",
    "            phrases_types.append(p_type)\n",
    "\n",
    "            observed_tags[tags_str] = p_type\n",
    "\n",
    "        else:\n",
    "            observed_tags[tags_str] = None\n",
    "\n",
    "#     try:\n",
    "#         Cache.save_observed_tags(observed_tags)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc(e)\n",
    "\n",
    "    return phrases_types, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents() + brown.sents() + reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 210608\n",
      "Words: 5503894\n",
      "Vocab: 87046\n"
     ]
    }
   ],
   "source": [
    "sents = [tuple(s) for s in sents]\n",
    "\n",
    "words = [w.lower() \n",
    "         for s in sents for w in s]\n",
    "\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "print('Sents:', len(sents))\n",
    "print('Words:', len(words))\n",
    "print('Vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indexes\n",
    "\n",
    "# {sentence: sent_index}\n",
    "sents_ind = {sents[i]: i for i in range(len(sents))}\n",
    "\n",
    "# {word: word_index}\n",
    "vocab_ind = {vocab[j]: j for j in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping from words to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: 58858\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in vocab]\n",
    "\n",
    "# {word_index: stem}\n",
    "stems_map = {vocab_ind[word]: stem \n",
    "             for word, stem in zip(vocab, stems)}\n",
    "\n",
    "print('Stems:', len(set(stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2word = {stem: vocab[ind]\n",
    "             for ind, stem in stems_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = gensim.models.Word2Vec.load('data/w2v/CBOW_300_10_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_stemmed = [[stems_map[vocab_ind[w.lower()]] for w in s] for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sents_stemmed, size=300, window=15, min_count=1, hs=1, negative=0)\n",
    "model.save('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.3553354740142822), ('daughter', 0.35418325662612915), ('esther', 0.3361766040325165), ('husband', 0.3246108889579773), ('absalom', 0.3200189769268036), ('mordecai', 0.31594496965408325), ('samaria', 0.3088769316673279), ('vashti', 0.30666205286979675), ('hebron', 0.298782080411911), ('selleth', 0.29791125655174255)]\n",
      "0.20989265750352995\n",
      "bring\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "print(model.wv.most_similar(positive=[stemmer.stem('woman'), stemmer.stem('king')], negative=[stemmer.stem('man')]))\n",
    "print(model.wv.similarity(stemmer.stem('campus'), stemmer.stem('dormitory')))\n",
    "print(model.wv.doesnt_match([stemmer.stem(w) for w in \"dormitory bring campus\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='rb') as fp:\n",
    "    stems_phrases = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = NGramsParser()\n",
    "\n",
    "sents_words_indexes = parser.parse_sents_tokens(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210608it [06:48, 515.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# {stem: Set[sentence_ind]}\n",
    "stems_phrases = defaultdict(dict)\n",
    "\n",
    "for i, (s_words, words_indexes) in tqdm(enumerate(zip(sents, sents_words_indexes), start=1)):\n",
    "    words_ttokens = nltk.pos_tag([w.lower() for w in s_words])\n",
    "    \n",
    "    tt_ngrams = [ngr\n",
    "                 for i in range(2, 5 + 1) \n",
    "                 for ngr in n_grams(words_ttokens, i, words_indexes, pad_left=False)]\n",
    "\n",
    "    types, phrases = parse_phrases(tt_ngrams)\n",
    "\n",
    "    # format and store phrases\n",
    "    for t, p in zip(types, phrases):\n",
    "        phrase_inds = tuple(vocab_ind[token] for token, _ in p)\n",
    "        \n",
    "        for word_ind in phrase_inds:\n",
    "            if vocab[word_ind] not in stop_words:\n",
    "                stem = stems_map[word_ind]\n",
    "                \n",
    "                if not stems_phrases[stem].get(t):\n",
    "                    stems_phrases[stem][t] = defaultdict(set)\n",
    "                \n",
    "                stem_phr_t = stems_phrases[stem][t]\n",
    "                \n",
    "                stem_phr_t[phrase_inds].add(sents_ind[s_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(word, stemmed=True):\n",
    "    if stemmed:\n",
    "        stem = word\n",
    "    else:\n",
    "        stem = stemmer.stem(word)\n",
    "        \n",
    "    phrs = stems_phrases[stem]\n",
    "    \n",
    "    if not phrs:\n",
    "        return {}\n",
    "    \n",
    "    phrases_ = defaultdict(set)\n",
    "    \n",
    "    for p_type, phr_dict in phrs.items():\n",
    "        for phrase, sents in phr_dict.items():\n",
    "            phrase_ = tuple(vocab[ind] for ind in phrase)\n",
    "            \n",
    "            phrases_[p_type].add(phrase_)\n",
    "            \n",
    "    return phrases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='wb') as fp:\n",
    "    pickle.dump(stems_phrases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for analysis\n",
    "\n",
    "phrases_count = defaultdict(dict)\n",
    "\n",
    "phrases_lengths_count = defaultdict(int)\n",
    "phrases_types_count = defaultdict(int)\n",
    "\n",
    "stems_phrases_count = defaultdict(dict)\n",
    "\n",
    "for stem, stem_phrases_dict in stems_phrases.items():\n",
    "    for phrase_type, phrases_dict in stem_phrases_dict.items():\n",
    "#         stem_phrase_type_count = sum([len(s_ids) for s_ids in phrases_dict.values()])\n",
    "        for phrase_tuple, s_ids in phrases_dict.items():\n",
    "            phrase_len = len(phrase_tuple)\n",
    "            phrase_sents_count = len(s_ids)\n",
    "            \n",
    "            if not stems_phrases_count[stem].get(phrase_len):\n",
    "                stems_phrases_count[stem][phrase_len] = defaultdict(int)\n",
    "                \n",
    "            # count number of such phrase type for given stem\n",
    "            stems_phrases_count[stem][phrase_len][phrase_type] += phrase_sents_count \n",
    "\n",
    "            # count total number of such phrase type\n",
    "            phrases_count[phrase_len][phrase_type] = phrases_count[phrase_len].get(phrase_type, 0) + phrase_sents_count\n",
    "\n",
    "            # count total number of all phrases\n",
    "            phrases_lengths_count[phrase_len] += phrase_sents_count\n",
    "            phrases_types_count[phrase_type] += phrase_sents_count\n",
    "\n",
    "all_phrases_count = sum(phrases_types_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {2: {'ADJP': 288142,\n",
       "              'ADVP': 418156,\n",
       "              'NP': 1985844,\n",
       "              'PP': 3328,\n",
       "              'VP': 66991},\n",
       "             3: {'ADJP': 44770,\n",
       "              'ADVP': 31064,\n",
       "              'NP': 1353053,\n",
       "              'PP': 7554,\n",
       "              'VP': 8215},\n",
       "             4: {'ADJP': 4179, 'ADVP': 223, 'NP': 429226},\n",
       "             5: {'ADJP': 245, 'NP': 53025}})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: defaultdict(int, {'ADJP': 12, 'ADVP': 8, 'NP': 28, 'VP': 9}),\n",
       " 3: defaultdict(int, {'ADJP': 1, 'NP': 12, 'VP': 1}),\n",
       " 4: defaultdict(int, {'NP': 4})}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = 'strive'\n",
    "\n",
    "stems_phrases_count[stemmer.stem(WORD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NP', 2), -12.029594340781607),\n",
       " (('ADJP', 2), -12.87689220116881),\n",
       " (('NP', 3), -12.876892201168811),\n",
       " (('VP', 2), -13.164574273620593),\n",
       " (('ADVP', 2), -13.282357309276973),\n",
       " (('NP', 4), -13.97550448983692),\n",
       " (('VP', 3), -15.361798850956813),\n",
       " (('ADJP', 3), -15.361798850956813)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrases_model(word, stemmed=True):\n",
    "    \"\"\"\n",
    "        Return sorted log probs of phrase types and length for the word \n",
    "    \"\"\"\n",
    "    if not stemmed:\n",
    "        stem = stemmer.stem(word)\n",
    "    else:\n",
    "        stem = word\n",
    "    \n",
    "    if not stems_phrases_count.get(stem):\n",
    "        return []\n",
    "    \n",
    "    stem_phrases = stems_phrases_count[stem]\n",
    "    \n",
    "    phrases_probs = []\n",
    "    \n",
    "    for p_length, p_types_count in stem_phrases.items():\n",
    "        for p_type, p_type_count in p_types_count.items():\n",
    "            prob_len = phrases_lengths_count[p_length] / all_phrases_count\n",
    "            \n",
    "            prob_len_type = phrases_count[p_length][p_type] / phrases_lengths_count[p_length]\n",
    "            \n",
    "            prob_word_len_type = p_type_count / phrases_count[p_length][p_type]\n",
    "            \n",
    "            log_prob = sum(np.log(p) for p in [prob_word_len_type, prob_len_type, prob_len])\n",
    "            \n",
    "            phrases_probs.append(((p_type, p_length), log_prob))\n",
    "    \n",
    "    phrases, scores = [], []\n",
    "    for phr, s in sorted(phrases_probs, key=itemgetter(1), reverse=True):\n",
    "        phrases.append(phr)\n",
    "        scores.append(s)\n",
    "    \n",
    "#     scores = np.exp(scores)\n",
    "#     scores /= np.max(scores)\n",
    "    \n",
    "    return list(zip(phrases, scores))\n",
    "\n",
    "phrases_model(WORD, stemmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring phrases (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.25\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 5\n",
    "\n",
    "P_TYPE_SCORE = dict(phrases_model(WORD, stemmed=False))\n",
    "\n",
    "phrases_ = get_phrases(WORD, stemmed=False)\n",
    "\n",
    "all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "for p_type, phrases in phrases_.items():\n",
    "    phrases_ = list(phrases)\n",
    "    \n",
    "    scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                   for phr in phrases_]))\n",
    "\n",
    "    for phr, sc in zip(phrases_, scores):\n",
    "        if not all_scored_phrases[p_type].get(len(phr)):\n",
    "            all_scored_phrases[p_type][len(phr)] = []\n",
    "        \n",
    "        score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "        \n",
    "        all_scored_phrases[p_type][len(phr)].append((phr, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP\n",
      "\n",
      "2\n",
      "[('strive against', 45.37863693436525),\n",
      " ('also strive', 45.6956552048578),\n",
      " ('thou strive', 51.02504954537294),\n",
      " ('economic strivings', 51.26184067925355),\n",
      " ('the strivings', 52.57374654015443)]\n",
      "\n",
      "3\n",
      "[('the time striving', 42.21950544921492),\n",
      " ('no man strive', 42.35136284438704),\n",
      " ('dost thou strive', 44.11854281035994),\n",
      " ('a striving after', 44.736132760921464),\n",
      " ('a critic striving', 47.06441893188094)]\n",
      "\n",
      "4\n",
      "[('social and economic strivings', 26.75404972739356),\n",
      " ('than a critic striving', 32.95443148322242),\n",
      " ('the potsherd strive with', 34.64571375556129),\n",
      " ('one powerful nation strives', 36.17675871558326)]\n",
      "\n",
      "ADVP\n",
      "\n",
      "2\n",
      "[('strive against', 47.570972129232146),\n",
      " ('strive with', 47.65035311830685),\n",
      " ('strive for', 56.87991182459042),\n",
      " ('striving after', 60.76540462625668),\n",
      " ('strive at', 72.63408748758481)]\n",
      "\n",
      "VP\n",
      "\n",
      "2\n",
      "[('striving against', 47.36485181683348),\n",
      " ('striving with', 47.444232805908186),\n",
      " ('striving for', 56.673791512191755),\n",
      " ('strive not', 57.704012020767195),\n",
      " ('striving in', 58.118454083145124)]\n",
      "\n",
      "3\n",
      "[('strive or fly', 53.73731564514743)]\n",
      "\n",
      "ADJP\n",
      "\n",
      "2\n",
      "[('also strive', 47.178426460535405),\n",
      " ('so strive', 51.986163334162114),\n",
      " ('so striving', 51.986163334162114),\n",
      " ('vainly striving', 56.30455656460035),\n",
      " ('strive not', 57.20056839397657)]\n",
      "\n",
      "3\n",
      "[('not always strive', 52.29377697937595)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    print('%s\\n' % p_type)\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "        print(n)\n",
    "        pprint(best_phr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ADJP': [('costly and taking', 43.71989976637133),\n",
       "              ('then taking', 46.55225528239214),\n",
       "              ('not taking', 48.7940656042),\n",
       "              ('take ye good', 49.20127879850634),\n",
       "              ('once taking', 49.73397696970903)],\n",
       "             'ADVP': [('taking over', 36.99610413129595),\n",
       "              ('take over', 36.99610413129595),\n",
       "              ('take of', 41.23225248868731),\n",
       "              ('of take', 41.23225248868731),\n",
       "              ('taking of', 41.23225248868731)],\n",
       "             'NP': [('this take constant vigilance .\"', 13.749093078121678),\n",
       "              (\"an employee's garden club take\", 14.565914176449315),\n",
       "              ('the same time taking', 21.42702209067359),\n",
       "              ('taking place', 24.3107733921767),\n",
       "              ('take places', 24.3107733921767)],\n",
       "             'PP': [('on its takings', 43.761827459264865)],\n",
       "             'VP': [('take away', 29.890197170588603),\n",
       "              ('takes away', 29.890197170588603),\n",
       "              ('taking over', 35.87874878725731),\n",
       "              ('go and take', 37.72527507815673),\n",
       "              ('take seriously', 38.14741386255943)]})"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group different length together\n",
    "\n",
    "candidate_phrases = defaultdict(list)\n",
    "\n",
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "        \n",
    "        candidate_phrases[p_type].extend(best_phr)\n",
    "        \n",
    "    # TODO: filter similar phrases and phrases with keywords\n",
    "    candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "    \n",
    "candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of scoring phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add probabilistic CFG\n",
    "\n",
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.5\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 5\n",
    "\n",
    "def get_scored_phrases(word, include_scores=True):\n",
    "    P_TYPE_SCORE = dict(phrases_model(word, stemmed=False))\n",
    "\n",
    "    phrases_ = get_phrases(word, stemmed=False)\n",
    "\n",
    "    all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "    for p_type, phrases in phrases_.items():\n",
    "        phrases_ = list(phrases)\n",
    "\n",
    "        scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                       for phr in phrases_]))\n",
    "\n",
    "        for phr, sc in zip(phrases_, scores):\n",
    "            if not all_scored_phrases[p_type].get(len(phr)):\n",
    "                all_scored_phrases[p_type][len(phr)] = []\n",
    "\n",
    "            score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "\n",
    "            all_scored_phrases[p_type][len(phr)].append((phr, score))\n",
    "\n",
    "    # group different length together\n",
    "\n",
    "    candidate_phrases = defaultdict(list)\n",
    "\n",
    "    for p_type, scored_phrases in all_scored_phrases.items():\n",
    "        for n in scored_phrases:\n",
    "            nps = scored_phrases[n]\n",
    "            best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "            best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "            candidate_phrases[p_type].extend(best_phr)\n",
    "\n",
    "        # TODO: filter similar phrases and phrases with keywords\n",
    "        candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "        \n",
    "        if not include_scores:\n",
    "            candidate_phrases[p_type] = [p for p, _ in candidate_phrases[p_type]]\n",
    "            \n",
    "    return candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  874\n"
     ]
    }
   ],
   "source": [
    "kws = get_test_keywords('data/lingualeo_words.csv')\n",
    "\n",
    "print('Keywords: ', len(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test keywords: ['eagerly', 'supply', 'arise', 'feed', 'emergence']\n",
      "Test keywords stems: ['eager', 'suppli', 'aris', 'feed', 'emerg']\n",
      "All kws found: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5\n",
    "np.random.seed = 0\n",
    "\n",
    "all_kws_found = False\n",
    "\n",
    "while not all_kws_found:    \n",
    "    test_kws = list(np.random.choice(list(kws), size=TEST_SIZE, replace=False))\n",
    "\n",
    "    print('Test keywords:', test_kws)\n",
    "\n",
    "    test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "    kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "    print('Test keywords stems:', test_kws)\n",
    "\n",
    "    all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    print('All kws found:', all_kws_found)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'eagerly, supply, arise, feed, emergence'\n"
     ]
    }
   ],
   "source": [
    "pprint(', '.join(['eagerly', 'supply', 'arise', 'feed', 'emergence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suppli', 'feed'), ('aris', 'emerg'), ('eager',)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cluster_keywords(kws):\n",
    "    pairs = []\n",
    "\n",
    "    for i, kw in enumerate(kws):\n",
    "        cp = list(kws)\n",
    "        cp.remove(kw)\n",
    "\n",
    "        dsts = w2v.distances(kw, cp)\n",
    "#         print(kw, dsts)\n",
    "        max_ind = np.argmin(dsts)\n",
    "#         print(i, max_ind)\n",
    "        sim_kw = kws[max_ind + (1 if i <= max_ind else 0)]\n",
    "#         print(kw, sim_kw)\n",
    "\n",
    "        pair = (kw, sim_kw)\n",
    "\n",
    "        if (sim_kw, kw) in pairs:\n",
    "            pair = (sim_kw, kw)\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "    final_clusters = []\n",
    "    \n",
    "    kws_ = list(kws)\n",
    "    \n",
    "    for kws_pair, cnt in Counter(pairs).items():\n",
    "        if cnt > 1:\n",
    "            final_clusters.append(kws_pair)\n",
    "            [kws_.remove(w) for w in kws_pair]\n",
    "        \n",
    "    return final_clusters + [(kw,) for kw in kws_]\n",
    "\n",
    "cluster_keywords(test_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('harsh', 'nutshel') 15060\n",
      "[(['so', 'harsh', ',', 'said', 'harsh'], -32.27874),\n",
      " (['so', 'harsh', 'for', 'that', 'harsh'], -32.34414),\n",
      " (['nutshel', ',\"', 'said', 'harsh', ','], -32.487923),\n",
      " (['nutshel', ',\"', ',', 'said', 'harsh'], -32.487923),\n",
      " (['a', 'nutshel', ',\"', 'said', 'harsh'], -32.695766),\n",
      " (['said', 'harsh', 'a', 'nutshel', ',\"'], -32.69577),\n",
      " (['grate', 'so', 'harsh', 'that', 'harsh'], -33.295624),\n",
      " (['said', 'harsh', 'in', 'a', 'harsh', 'voic'], -34.27955),\n",
      " (['in', 'a', 'harsh', 'voic', 'said', 'harsh'], -34.27955),\n",
      " (['the', 'harsh', 'voic', 'of', 'said', 'harsh'], -34.46181)]\n",
      "\n",
      "('essenti', 'implicit') 121171\n",
      "[(['the', 'essenti', 'foreign', 'exchang', 'requir'], -28.382555),\n",
      " (['all', 'the', 'essenti', 'are', 'essenti'], -28.967785),\n",
      " (['essenti', 'within', 'the', 'are', 'essenti'], -29.271837),\n",
      " (['essenti', 'of', 'a', 'is', 'essenti'], -29.710833),\n",
      " (['not', 'essenti', ',', 'are', 'essenti'], -30.0416),\n",
      " (['essenti', 'not', ',', 'are', 'essenti'], -30.0416),\n",
      " (['essenti', 'within', 'the', 'is', 'essenti'], -30.063168),\n",
      " (['the', 'essenti', 'differ', 'between', 'is', 'essenti'], -30.445156),\n",
      " (['is', 'essenti', 'the', 'essenti', 'differ', 'between'], -30.445158),\n",
      " (['not', 'essenti', ',', 'is', 'essenti'], -30.480988)]\n",
      "\n",
      "('hold',) 20800\n",
      "[(['the', 'hold', 'hold', 'plc', '&', 'lt'], -19.603794),\n",
      " (['hold', 'plc', '&', 'lt', 'the', 'hold'], -19.603796),\n",
      " (['hold', 'plc', '&', 'lt', 'on', 'hold'], -21.38333),\n",
      " (['hold', 'plc', '&', 'lt', 'hold', 'on'], -21.38333),\n",
      " (['hold', 'plc', '&', 'lt', 'hold', 'on'], -21.38333),\n",
      " (['on', 'hold', 'hold', 'plc', '&', 'lt'], -21.383331),\n",
      " (['hold', 'on', 'hold', 'plc', '&', 'lt'], -21.383331),\n",
      " (['hold', 'on', 'hold', 'plc', '&', 'lt'], -21.383331),\n",
      " (['hold', 'firm', 'hold', 'plc', '&', 'lt'], -22.19986),\n",
      " (['hold', 'plc', '&', 'lt', 'firm', 'hold'], -22.19986)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kws_tuple in cluster_keywords(test_kws):\n",
    "    sents = gen_sents_candidates(kws_tuple, [get_scored_phrases(kw, include_scores=False) for kw in kws_tuple])\n",
    "    \n",
    "    ranked_sents = rank_sents(sents)\n",
    "    \n",
    "    print(kws_tuple, len(sents))\n",
    "    pprint(ranked_sents)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_rules = Cache.load_terminal_rules_sents()\n",
    "\n",
    "def rank_sents(sents, top_n=10):\n",
    "    sents = [[stems_map.get(vocab_ind.get(w, w), w) \n",
    "              for w in s.split(' ')] \n",
    "             for s in sents\n",
    "            if len(s.split(' ')) > 4]\n",
    "    return sorted(zip(sents, model.score(sents)), key=itemgetter(1), reverse=True)[:top_n]\n",
    "    \n",
    "def combine_elements(*args):\n",
    "    return list(product(*args, repeat=1))\n",
    "\n",
    "def gen_sents_candidates(kws, kws_phrases):\n",
    "    marks = {'COMMA': [\",\"],\n",
    "             'COLON': [\":\"],\n",
    "             'SEMICOLON': [\";\"],\n",
    "             # 'DOT': [\".\"],\n",
    "             'QUESTION': [\"?\"],\n",
    "             'EXCLAM': [\"!\"],\n",
    "             'DASH': [\"-\"]}\n",
    "\n",
    "    sents = set()\n",
    "\n",
    "    phrases = defaultdict(dict)\n",
    "\n",
    "    for kw, kw_phrases in zip(kws, kws_phrases):\n",
    "        for p_type in kw_phrases:\n",
    "            phrases[p_type][kw] = kw_phrases[p_type]\n",
    "\n",
    "    for sents_rule in sents_rules:\n",
    "        sent_symbols = str2ngram(sents_rule)\n",
    "        \n",
    "        sent_phrases = []\n",
    "        \n",
    "        # form lists of each phrase\n",
    "        for p_type in sent_symbols:\n",
    "            if p_type not in marks:\n",
    "                sent_phrases.append([phrases[p_type].get(kw) \n",
    "                                     for kw in kws\n",
    "                                    if phrases[p_type].get(kw)])\n",
    "            else:\n",
    "                sent_phrases.append(marks.get(p_type, []))\n",
    "            \n",
    "        # combine phrases of keywords\n",
    "        kws_phrases_product = combine_elements(*sent_phrases)\n",
    "        \n",
    "        # combine sents\n",
    "        for kws_comb in kws_phrases_product:\n",
    "            sents_candidates = [' '.join(s) for s in combine_elements(*kws_comb)]\n",
    "            \n",
    "            sents.update(sents_candidates)\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using processed corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "t = 'grievance'\n",
    "\n",
    "s = stemmer.stem(t)\n",
    "# stems_phrases[s]\n",
    "vocab_ind['grievance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "t = 'grievance'\n",
    "\n",
    "s = stemmer.stem(t)\n",
    "\n",
    "ss = []\n",
    "for k, v in stems_phrases[s]['NP'].items():\n",
    "    ss.extend([s_id for s_id in v])\n",
    "    \n",
    "Counter(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
