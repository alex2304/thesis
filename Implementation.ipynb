{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "\n",
    "from utils.nltk_utils import *\n",
    "from utils.ngrams import *\n",
    "from utils.eval import get_test_keywords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.grammar_utils import tags_seq_to_symbols\n",
    "from utils.io import Cache\n",
    "from utils.ngrams import ngram2str\n",
    "\n",
    "\n",
    "observed_tags = Cache.load_observed_tags()\n",
    "terminal_rules = Cache.load_terminal_rules()\n",
    "\n",
    "\n",
    "def parse_phrases(tt_ngrams) -> Tuple[List, List[List[Tuple]]]:\n",
    "    global observed_tags, terminal_rules\n",
    "    \n",
    "    phrases = []\n",
    "    phrases_types = []\n",
    "\n",
    "    if observed_tags is None:\n",
    "        observed_tags = dict()\n",
    "\n",
    "    for tt_gram in tt_ngrams:\n",
    "        symbols = tuple(tags_seq_to_symbols([tag\n",
    "                                             for _, tag in tt_gram]))\n",
    "\n",
    "        phrase = tt_gram\n",
    "\n",
    "        # check if tags phrase has been already observed\n",
    "        tags_str = ngram2str(symbols)\n",
    "\n",
    "        if tags_str in observed_tags:\n",
    "            if observed_tags[tags_str] is not None:\n",
    "                phrases.append(phrase)\n",
    "                phrases_types.append(observed_tags[tags_str])\n",
    "\n",
    "            continue\n",
    "\n",
    "        p_types_dict = terminal_rules.get(tags_str)\n",
    "\n",
    "        if p_types_dict:\n",
    "            p_type = max(p_types_dict, key=lambda k: p_types_dict[k])\n",
    "\n",
    "            phrases.append(phrase)\n",
    "            phrases_types.append(p_type)\n",
    "\n",
    "            observed_tags[tags_str] = p_type\n",
    "\n",
    "        else:\n",
    "            observed_tags[tags_str] = None\n",
    "\n",
    "#     try:\n",
    "#         Cache.save_observed_tags(observed_tags)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc(e)\n",
    "\n",
    "    return phrases_types, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents() + brown.sents() + reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 210608\n",
      "Words: 5503894\n",
      "Vocab: 87046\n"
     ]
    }
   ],
   "source": [
    "sents = [tuple(s) for s in sents]\n",
    "\n",
    "words = [w.lower() \n",
    "         for s in sents for w in s]\n",
    "\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "print('Sents:', len(sents))\n",
    "print('Words:', len(words))\n",
    "print('Vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indexes\n",
    "\n",
    "# {sentence: sent_index}\n",
    "sents_ind = {sents[i]: i for i in range(len(sents))}\n",
    "\n",
    "# {word: word_index}\n",
    "vocab_ind = {vocab[j]: j for j in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping from words to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: 58858\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in vocab]\n",
    "\n",
    "# {word_index: stem}\n",
    "stems_map = {vocab_ind[word]: stem \n",
    "             for word, stem in zip(vocab, stems)}\n",
    "\n",
    "print('Stems:', len(set(stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2word = {stem: vocab[ind]\n",
    "             for ind, stem in stems_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = gensim.models.Word2Vec.load('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_stemmed = [[stems_map[vocab_ind[w.lower()]] for w in s] for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sents_stemmed, size=300, window=15, min_count=1, hs=1, negative=0)\n",
    "model.save('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "print(model.wv.most_similar(positive=[stemmer.stem('woman'), stemmer.stem('king')], negative=[stemmer.stem('man')]))\n",
    "print(model.wv.similarity(stemmer.stem('campus'), stemmer.stem('dormitory')))\n",
    "print(model.wv.doesnt_match([stemmer.stem(w) for w in \"dormitory bring campus\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='rb') as fp:\n",
    "    stems_phrases = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = NGramsParser()\n",
    "\n",
    "sents_words_indexes = parser.parse_sents_tokens(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {stem: Set[sentence_ind]}\n",
    "stems_phrases = defaultdict(dict)\n",
    "\n",
    "for i, (s_words, words_indexes) in tqdm(enumerate(zip(sents, sents_words_indexes), start=1)):\n",
    "    words_ttokens = nltk.pos_tag([w.lower() for w in s_words])\n",
    "    \n",
    "    tt_ngrams = [ngr\n",
    "                 for i in range(2, 5 + 1) \n",
    "                 for ngr in n_grams(words_ttokens, i, words_indexes, pad_left=False)]\n",
    "\n",
    "    types, phrases = parse_phrases(tt_ngrams)\n",
    "\n",
    "    # format and store phrases\n",
    "    for t, p in zip(types, phrases):\n",
    "        phrase_inds = tuple(vocab_ind[token] for token, _ in p)\n",
    "        \n",
    "        for word_ind in phrase_inds:\n",
    "            if vocab[word_ind] not in stop_words:\n",
    "                stem = stems_map[word_ind]\n",
    "                \n",
    "                if not stems_phrases[stem].get(t):\n",
    "                    stems_phrases[stem][t] = defaultdict(set)\n",
    "                \n",
    "                stem_phr_t = stems_phrases[stem][t]\n",
    "                \n",
    "                stem_phr_t[phrase_inds].add(sents_ind[s_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJP': defaultdict(set,\n",
       "             {(5566, 8631): {53379},\n",
       "              (6509, 8629): {155524},\n",
       "              (8629, 8322): {100786, 136405, 136436},\n",
       "              (8629, 10767): {142473},\n",
       "              (8629, 14426): {129262, 163315, 166916, 188102},\n",
       "              (8629, 31882): {128992},\n",
       "              (8629, 40323): {107429, 118657, 129508, 132871, 133063, 133149},\n",
       "              (8629, 77432): {103574,\n",
       "               129208,\n",
       "               129583,\n",
       "               129699,\n",
       "               130147,\n",
       "               130238,\n",
       "               130557,\n",
       "               131722,\n",
       "               133059,\n",
       "               133415,\n",
       "               134962,\n",
       "               136578,\n",
       "               143528,\n",
       "               155524,\n",
       "               164946},\n",
       "              (10042, 8629): {122610},\n",
       "              (16046, 8629): {136027},\n",
       "              (22386, 8629): {163315, 166916},\n",
       "              (28806, 8631): {78913},\n",
       "              (54085, 8631): {188105},\n",
       "              (54928, 8631): {131253},\n",
       "              (75026, 8629): {62560},\n",
       "              (76214, 8629): {134962},\n",
       "              (79104, 8629): {133063}}),\n",
       " 'ADVP': defaultdict(set,\n",
       "             {(8628, 77456): {191787},\n",
       "              (70362, 8628): {191787},\n",
       "              (77542, 8630): {89778},\n",
       "              (77736, 8630): {195255}}),\n",
       " 'NP': defaultdict(set,\n",
       "             {(1677, 53178, 8630): {195818},\n",
       "              (4142, 79104, 8629): {133063},\n",
       "              (6425, 8628): {188476},\n",
       "              (6841, 8629): {67418, 129544, 141032},\n",
       "              (6841, 8629, 5870): {67418},\n",
       "              (6841, 8629, 5870, 54834): {67418},\n",
       "              (6841, 8629, 40042): {129544},\n",
       "              (6841, 8629, 40042, 82795): {129544},\n",
       "              (6841, 8629, 52570): {141032},\n",
       "              (6958, 6841, 8629, 40042): {129544},\n",
       "              (8628, 4142): {10854,\n",
       "               90541,\n",
       "               91285,\n",
       "               97039,\n",
       "               102810,\n",
       "               124984,\n",
       "               128804,\n",
       "               129251,\n",
       "               146735,\n",
       "               192731,\n",
       "               210054},\n",
       "              (8628, 4425): {167315},\n",
       "              (8628, 6620): {188470, 188472},\n",
       "              (8628, 6841): {136474},\n",
       "              (8628, 7409): {117845, 188476},\n",
       "              (8628, 24096): {179547},\n",
       "              (8628, 43651): {133304},\n",
       "              (8628, 48292): {85693},\n",
       "              (8628, 53631): {133114},\n",
       "              (8628, 54885): {123829},\n",
       "              (8628, 72194): {166865},\n",
       "              (8628, 77432): {97073,\n",
       "               104759,\n",
       "               105355,\n",
       "               116435,\n",
       "               120802,\n",
       "               121453,\n",
       "               124985,\n",
       "               125318,\n",
       "               128875,\n",
       "               129190,\n",
       "               129706,\n",
       "               131994,\n",
       "               132229,\n",
       "               132819,\n",
       "               132875,\n",
       "               134690,\n",
       "               135168,\n",
       "               142539,\n",
       "               150797,\n",
       "               163081,\n",
       "               181475},\n",
       "              (8628, 77456): {187,\n",
       "               66413,\n",
       "               78058,\n",
       "               81366,\n",
       "               115127,\n",
       "               120880,\n",
       "               125006,\n",
       "               131072,\n",
       "               133693,\n",
       "               133696,\n",
       "               144948,\n",
       "               157951,\n",
       "               159366,\n",
       "               178610,\n",
       "               182963,\n",
       "               197604,\n",
       "               200077,\n",
       "               203270,\n",
       "               203391},\n",
       "              (8628, 77604): {85527},\n",
       "              (8628, 78866): {70195, 74759},\n",
       "              (8629, 2335): {158434},\n",
       "              (8629, 5870): {67418},\n",
       "              (8629, 5870, 54834): {67418},\n",
       "              (8629, 9098): {131420},\n",
       "              (8629, 19116): {136027},\n",
       "              (8629, 19830): {186527},\n",
       "              (8629, 20598): {179758},\n",
       "              (8629, 36199): {193280},\n",
       "              (8629, 40042): {129544},\n",
       "              (8629, 40042, 82795): {129544},\n",
       "              (8629, 52570): {141032},\n",
       "              (8629, 65152): {127306},\n",
       "              (8630, 72194): {89778},\n",
       "              (8630, 72194, 55785): {89778},\n",
       "              (8630, 72194, 55785, 38739, 32130): {89778},\n",
       "              (8631, 5414): {129133, 136426},\n",
       "              (8631, 6620): {188480},\n",
       "              (8631, 7074, 19629): {112714},\n",
       "              (8631, 9098): {87054},\n",
       "              (8631, 15479): {178064},\n",
       "              (8631, 15479, 41682): {178064},\n",
       "              (8631, 19905): {205379},\n",
       "              (8631, 20059): {191693},\n",
       "              (8631, 20059, 60864): {191693},\n",
       "              (8631, 21209): {180864},\n",
       "              (8631, 21209, 54955): {180752},\n",
       "              (8631, 29071, 19830): {125048},\n",
       "              (8631, 29135): {206542},\n",
       "              (8631, 35292): {82960},\n",
       "              (8631, 40194): {163802},\n",
       "              (8631, 48669): {204740},\n",
       "              (8631, 48669, 19116): {204740},\n",
       "              (8631, 54955): {180364, 203646, 209266},\n",
       "              (8631, 54955, 60864): {180364, 203646, 209266},\n",
       "              (8631, 65152): {128765},\n",
       "              (8631, 72837): {129581},\n",
       "              (8631, 77571, 28255): {129118},\n",
       "              (16046, 8629, 19116): {136027},\n",
       "              (28852, 51453, 8631): {82960},\n",
       "              (28852, 51453, 8631, 35292): {82960},\n",
       "              (30153, 8630): {124887},\n",
       "              (49851, 8628): {81366},\n",
       "              (51453, 8631): {82960},\n",
       "              (51453, 8631, 35292): {82960},\n",
       "              (53178, 8630): {195818},\n",
       "              (53251, 8628): {78058},\n",
       "              (53261, 73393, 8631): {128765},\n",
       "              (54085, 8628): {97073, 117626, 192731},\n",
       "              (54194, 8628): {181475},\n",
       "              (62471, 8628): {102810},\n",
       "              (65947, 8631): {108029},\n",
       "              (68158, 8630): {97769},\n",
       "              (69369, 8628): {85348},\n",
       "              (71047, 8628): {86143},\n",
       "              (73393, 8631): {128765},\n",
       "              (73393, 8631, 65152): {128765},\n",
       "              (77432, 8629): {129262},\n",
       "              (77432, 12992, 8629): {85266},\n",
       "              (77456, 41451, 8629): {204205},\n",
       "              (77456, 53261, 73393, 8631): {128765},\n",
       "              (77456, 62432, 8629): {79529},\n",
       "              (77456, 82556, 8629): {136936},\n",
       "              (77791, 8629): {118657},\n",
       "              (78030, 8628): {136474},\n",
       "              (80827, 6841, 8629, 52570): {141032},\n",
       "              (85626, 6841, 8629, 5870): {67418}}),\n",
       " 'VP': defaultdict(set,\n",
       "             {(8628, 361, 47368): {158610},\n",
       "              (8629, 6352): {153680},\n",
       "              (8631, 31882): {79580},\n",
       "              (8631, 77432): {108029, 129016, 132751, 177963}})}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_phrases(word, stemmed=True):\n",
    "    if stemmed:\n",
    "        stem = word\n",
    "    else:\n",
    "        stem = stemmer.stem(word)\n",
    "        \n",
    "    phrs = stems_phrases[stem]\n",
    "    \n",
    "    if not phrs:\n",
    "        return {}\n",
    "    \n",
    "    phrases_ = defaultdict(set)\n",
    "    \n",
    "    for p_type, phr_dict in phrs.items():\n",
    "        for phrase, sents in phr_dict.items():\n",
    "            phrase_ = tuple(vocab[ind] for ind in phrase)\n",
    "            \n",
    "            phrases_[p_type].add(phrase_)\n",
    "            \n",
    "    return phrases_\n",
    "\n",
    "stems_phrases['assum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='wb') as fp:\n",
    "    pickle.dump(stems_phrases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for analysis\n",
    "\n",
    "phrases_count = defaultdict(dict)\n",
    "\n",
    "phrases_lengths_count = defaultdict(int)\n",
    "phrases_types_count = defaultdict(int)\n",
    "\n",
    "stems_phrases_count = defaultdict(dict)\n",
    "\n",
    "for stem, stem_phrases_dict in stems_phrases.items():\n",
    "    for phrase_type, phrases_dict in stem_phrases_dict.items():\n",
    "#         stem_phrase_type_count = sum([len(s_ids) for s_ids in phrases_dict.values()])\n",
    "        for phrase_tuple, s_ids in phrases_dict.items():\n",
    "            phrase_len = len(phrase_tuple)\n",
    "            phrase_sents_count = len(s_ids)\n",
    "            \n",
    "            if not stems_phrases_count[stem].get(phrase_len):\n",
    "                stems_phrases_count[stem][phrase_len] = defaultdict(int)\n",
    "                \n",
    "            # count number of such phrase type for given stem\n",
    "            stems_phrases_count[stem][phrase_len][phrase_type] += phrase_sents_count \n",
    "\n",
    "            # count total number of such phrase type\n",
    "            phrases_count[phrase_len][phrase_type] = phrases_count[phrase_len].get(phrase_type, 0) + phrase_sents_count\n",
    "\n",
    "            # count total number of all phrases\n",
    "            phrases_lengths_count[phrase_len] += phrase_sents_count\n",
    "            phrases_types_count[phrase_type] += phrase_sents_count\n",
    "\n",
    "all_phrases_count = sum(phrases_types_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4694015"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: defaultdict(int, {'ADJP': 12, 'ADVP': 8, 'NP': 28, 'VP': 9}),\n",
       " 3: defaultdict(int, {'ADJP': 1, 'NP': 12, 'VP': 1}),\n",
       " 4: defaultdict(int, {'NP': 4})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = 'strive'\n",
    "\n",
    "stems_phrases_count[stemmer.stem(WORD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NP', 2), -12.029594340781607),\n",
       " (('ADJP', 2), -12.87689220116881),\n",
       " (('NP', 3), -12.876892201168811),\n",
       " (('VP', 2), -13.164574273620593),\n",
       " (('ADVP', 2), -13.282357309276973),\n",
       " (('NP', 4), -13.97550448983692),\n",
       " (('VP', 3), -15.361798850956813),\n",
       " (('ADJP', 3), -15.361798850956813)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrases_model(word, stemmed=True):\n",
    "    \"\"\"\n",
    "        Return sorted log probs of phrase types and length for the word \n",
    "    \"\"\"\n",
    "    if not stemmed:\n",
    "        stem = stemmer.stem(word)\n",
    "    else:\n",
    "        stem = word\n",
    "    \n",
    "    if not stems_phrases_count.get(stem):\n",
    "        return []\n",
    "    \n",
    "    stem_phrases = stems_phrases_count[stem]\n",
    "    \n",
    "    phrases_probs = []\n",
    "    \n",
    "    for p_length, p_types_count in stem_phrases.items():\n",
    "        for p_type, p_type_count in p_types_count.items():\n",
    "            prob_len = phrases_lengths_count[p_length] / all_phrases_count\n",
    "            \n",
    "            prob_len_type = phrases_count[p_length][p_type] / phrases_lengths_count[p_length]\n",
    "            \n",
    "            prob_word_len_type = p_type_count / phrases_count[p_length][p_type]\n",
    "            \n",
    "            log_prob = sum(np.log(p) for p in [prob_word_len_type, prob_len_type, prob_len])\n",
    "            \n",
    "            phrases_probs.append(((p_type, p_length), log_prob))\n",
    "    \n",
    "    phrases, scores = [], []\n",
    "    for phr, s in sorted(phrases_probs, key=itemgetter(1), reverse=True):\n",
    "        phrases.append(phr)\n",
    "        scores.append(s)\n",
    "    \n",
    "#     scores = np.exp(scores)\n",
    "#     scores /= np.max(scores)\n",
    "    \n",
    "    return list(zip(phrases, scores))\n",
    "\n",
    "phrases_model(WORD, stemmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring phrases (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.25\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 5\n",
    "\n",
    "P_TYPE_SCORE = dict(phrases_model(WORD, stemmed=False))\n",
    "\n",
    "phrases_ = get_phrases(WORD, stemmed=False)\n",
    "\n",
    "all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "for p_type, phrases in phrases_.items():\n",
    "    phrases_ = list(phrases)\n",
    "    \n",
    "    scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                   for phr in phrases_]))\n",
    "\n",
    "    for phr, sc in zip(phrases_, scores):\n",
    "        if not all_scored_phrases[p_type].get(len(phr)):\n",
    "            all_scored_phrases[p_type][len(phr)] = []\n",
    "        \n",
    "        score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "        \n",
    "        all_scored_phrases[p_type][len(phr)].append((phr, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVP\n",
      "\n",
      "2\n",
      "[('strive against', 47.570972129232146),\n",
      " ('strive with', 47.65035311830685),\n",
      " ('strive for', 56.87991182459042),\n",
      " ('striving after', 60.76540462625668),\n",
      " ('strive at', 72.63408748758481)]\n",
      "\n",
      "VP\n",
      "\n",
      "2\n",
      "[('striving against', 47.36485181683348),\n",
      " ('striving with', 47.444232805908186),\n",
      " ('striving for', 56.673791512191755),\n",
      " ('strive not', 57.704012020767195),\n",
      " ('striving in', 58.118454083145124)]\n",
      "\n",
      "3\n",
      "[('strive or fly', 53.73731564514743)]\n",
      "\n",
      "NP\n",
      "\n",
      "2\n",
      "[('strive against', 45.37863693436525),\n",
      " ('also strive', 45.6956552048578),\n",
      " ('thou strive', 51.02504954537294),\n",
      " ('economic strivings', 51.26184067925355),\n",
      " ('the strivings', 52.57374654015443)]\n",
      "\n",
      "3\n",
      "[('the time striving', 42.21950544921492),\n",
      " ('no man strive', 42.35136284438704),\n",
      " ('dost thou strive', 44.11854281035994),\n",
      " ('a striving after', 44.736132760921464),\n",
      " ('a critic striving', 47.06441893188094)]\n",
      "\n",
      "4\n",
      "[('social and economic strivings', 26.75404972739356),\n",
      " ('than a critic striving', 32.95443148322242),\n",
      " ('the potsherd strive with', 34.64571375556129),\n",
      " ('one powerful nation strives', 36.17675871558326)]\n",
      "\n",
      "ADJP\n",
      "\n",
      "2\n",
      "[('also strive', 47.178426460535405),\n",
      " ('so striving', 51.986163334162114),\n",
      " ('so strive', 51.986163334162114),\n",
      " ('vainly striving', 56.30455656460035),\n",
      " ('not striving', 57.20056839397657)]\n",
      "\n",
      "3\n",
      "[('not always strive', 52.29377697937595)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    print('%s\\n' % p_type)\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "        print(n)\n",
    "        pprint(best_phr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ADJP': [('also strive', 47.178426460535405),\n",
       "              ('so striving', 51.986163334162114),\n",
       "              ('so strive', 51.986163334162114),\n",
       "              ('not always strive', 52.29377697937595),\n",
       "              ('vainly striving', 56.30455656460035)],\n",
       "             'ADVP': [('strive against', 47.570972129232146),\n",
       "              ('strive with', 47.65035311830685),\n",
       "              ('strive for', 56.87991182459042),\n",
       "              ('striving after', 60.76540462625668),\n",
       "              ('strive at', 72.63408748758481)],\n",
       "             'NP': [('social and economic strivings', 26.75404972739356),\n",
       "              ('than a critic striving', 32.95443148322242),\n",
       "              ('the potsherd strive with', 34.64571375556129),\n",
       "              ('one powerful nation strives', 36.17675871558326),\n",
       "              ('the time striving', 42.21950544921492)],\n",
       "             'VP': [('striving against', 47.36485181683348),\n",
       "              ('striving with', 47.444232805908186),\n",
       "              ('strive or fly', 53.73731564514743),\n",
       "              ('striving for', 56.673791512191755),\n",
       "              ('strive not', 57.704012020767195)]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group different length together\n",
    "\n",
    "candidate_phrases = defaultdict(list)\n",
    "\n",
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "        \n",
    "        candidate_phrases[p_type].extend(best_phr)\n",
    "        \n",
    "    # TODO: filter similar phrases and phrases with keywords\n",
    "    candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "    \n",
    "candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of scoring phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add probabilistic CFG\n",
    "\n",
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.5\n",
    "}\n",
    "\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 10\n",
    "\n",
    "def get_scored_phrases(word, include_scores=True):\n",
    "    P_TYPE_SCORE = dict(phrases_model(word, stemmed=False))\n",
    "\n",
    "    phrases_ = get_phrases(word, stemmed=False)\n",
    "\n",
    "    all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "    for p_type, phrases in phrases_.items():\n",
    "        phrases_ = list(phrases)\n",
    "\n",
    "        scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                       for phr in phrases_]))\n",
    "\n",
    "        for phr, sc in zip(phrases_, scores):\n",
    "            if not all_scored_phrases[p_type].get(len(phr)):\n",
    "                all_scored_phrases[p_type][len(phr)] = []\n",
    "\n",
    "            score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "\n",
    "            all_scored_phrases[p_type][len(phr)].append((phr, score))\n",
    "\n",
    "    # group different length together\n",
    "\n",
    "    candidate_phrases = defaultdict(list)\n",
    "\n",
    "    for p_type, scored_phrases in all_scored_phrases.items():\n",
    "        for n in scored_phrases:\n",
    "            nps = scored_phrases[n]\n",
    "            best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "            best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "            candidate_phrases[p_type].extend(best_phr)\n",
    "\n",
    "        # TODO: filter similar phrases and phrases with keywords\n",
    "        candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)#[:TOP_N]\n",
    "        \n",
    "        if not include_scores:\n",
    "            candidate_phrases[p_type] = [p for p, _ in candidate_phrases[p_type]]\n",
    "            \n",
    "    return candidate_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(5566, 8631): {53379},\n",
       "             (6509, 8629): {155524},\n",
       "             (8629, 8322): {100786, 136405, 136436},\n",
       "             (8629, 10767): {142473},\n",
       "             (8629, 14426): {129262, 163315, 166916, 188102},\n",
       "             (8629, 31882): {128992},\n",
       "             (8629, 40323): {107429, 118657, 129508, 132871, 133063, 133149},\n",
       "             (8629, 77432): {103574,\n",
       "              129208,\n",
       "              129583,\n",
       "              129699,\n",
       "              130147,\n",
       "              130238,\n",
       "              130557,\n",
       "              131722,\n",
       "              133059,\n",
       "              133415,\n",
       "              134962,\n",
       "              136578,\n",
       "              143528,\n",
       "              155524,\n",
       "              164946},\n",
       "             (10042, 8629): {122610},\n",
       "             (16046, 8629): {136027},\n",
       "             (22386, 8629): {163315, 166916},\n",
       "             (28806, 8631): {78913},\n",
       "             (54085, 8631): {188105},\n",
       "             (54928, 8631): {131253},\n",
       "             (75026, 8629): {62560},\n",
       "             (76214, 8629): {134962},\n",
       "             (79104, 8629): {133063}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_phrases['assum']['ADJP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ' '.join(nltk.flatten(sents))\n",
    "\n",
    "escape_chars = ['æ', 'æ','è','é','î','ü', '\\n', '\\x1a']\n",
    "\n",
    "for c in escape_chars:\n",
    "    data = data.replace(c, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_LENGTH = 100 \n",
      "Number of sequences: 268435 \n",
      "VOCAB_SIZE = 92\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 100\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "SEQS_NUM = int(np.floor(len(data)/SEQ_LENGTH))\n",
    "\n",
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "print('SEQ_LENGTH =', SEQ_LENGTH, '\\nNumber of sequences:', SEQS_NUM, '\\nVOCAB_SIZE =', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 500\n",
    "LAYER_NUM = 3\n",
    "\n",
    "rnn = Sequential()\n",
    "\n",
    "rnn.add(layers.LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    rnn.add(layers.LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "\n",
    "rnn.add(layers.TimeDistributed(layers.Dense(VOCAB_SIZE)))\n",
    "rnn.add(layers.Activation('softmax'))\n",
    "\n",
    "rnn.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "rnn.load_weights('trained_large/checkpoint_500_all_epoch_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to recreational facilitiess^wWKr'w}1hw/Uhdw}^wgfw]w]w-"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-778137a350d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgenerate_from_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to recreational facilities'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-778137a350d9>\u001b[0m in \u001b[0;36mgenerate_from_context\u001b[0;34m(model, context, length)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1064\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_from_context(mdl, context, length=100):\n",
    "    ix = [char_to_ix[c] for c in context]\n",
    "\n",
    "    text = [c for c in context]\n",
    "\n",
    "    X = np.zeros((1, length + len(context), VOCAB_SIZE))\n",
    "\n",
    "    for i, ind in enumerate(ix):\n",
    "        X[0, i, :][ind] = 1\n",
    "\n",
    "        print(ix_to_char[ind], end=\"\")\n",
    "\n",
    "    for i in range(len(context), length + len(context)):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "\n",
    "        ix = np.argmax(mdl.predict(X[:, :i+1, :])[0], 1)\n",
    "\n",
    "        text.append(ix_to_char[ix[-1]])\n",
    "\n",
    "    return ('').join(text)\n",
    "  \n",
    "generate_from_context(rnn, 'to recreational facilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  874\n"
     ]
    }
   ],
   "source": [
    "kws = get_test_keywords('data/lingualeo_words.csv')\n",
    "\n",
    "print('Keywords: ', len(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_kws(cnt):\n",
    "    all_kws_found = False\n",
    "\n",
    "    while not all_kws_found:    \n",
    "        test_kws = list(np.random.choice(list(kws), size=cnt, replace=False))\n",
    "\n",
    "        word_kws = list(test_kws)\n",
    "\n",
    "        test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "        kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "\n",
    "        all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    return test_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test keywords: ['pointwise', 'caveat', 'tuned', 'recitation', 'ensure']\n",
      "Test keywords stems: ['pointwis', 'caveat', 'tune', 'recit', 'ensur']\n",
      "All kws found: False\n",
      "\n",
      "Test keywords: ['literally', 'denote', 'exhaustive', 'sleazy', 'rid']\n",
      "Test keywords stems: ['liter', 'denot', 'exhaust', 'sleazi', 'rid']\n",
      "All kws found: False\n",
      "\n",
      "Test keywords: ['conveying', 'rope', 'vesicle', 'dissent', 'brief']\n",
      "Test keywords stems: ['convey', 'rope', 'vesicl', 'dissent', 'brief']\n",
      "All kws found: False\n",
      "\n",
      "Test keywords: ['frown', 'dangle', 'turn', 'improper', 'various']\n",
      "Test keywords stems: ['frown', 'dangl', 'turn', 'improp', 'various']\n",
      "All kws found: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5\n",
    "np.random.seed = 0\n",
    "\n",
    "all_kws_found = False\n",
    "\n",
    "while not all_kws_found:    \n",
    "    test_kws = list(np.random.choice(list(kws), size=TEST_SIZE, replace=False))\n",
    "\n",
    "    word_kws = list(test_kws)\n",
    "    \n",
    "    print('Test keywords:', word_kws)\n",
    "    \n",
    "    test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "    kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "    print('Test keywords stems:', test_kws)\n",
    "\n",
    "    all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    print('All kws found:', all_kws_found)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('improp', 'various'), ('frown', 'dangl'), ('turn',)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cluster_keywords(kws):\n",
    "    pairs = []\n",
    "\n",
    "    for i, kw in enumerate(kws):\n",
    "        cp = list(kws)\n",
    "        cp.remove(kw)\n",
    "\n",
    "        dsts = w2v.distances(kw, cp)\n",
    "#         print(kw, dsts)\n",
    "        max_ind = np.argmin(dsts)\n",
    "#         print(i, max_ind)\n",
    "        sim_kw = kws[max_ind + (1 if i <= max_ind else 0)]\n",
    "#         print(kw, sim_kw)\n",
    "\n",
    "        pair = (kw, sim_kw)\n",
    "\n",
    "        if (sim_kw, kw) in pairs:\n",
    "            pair = (sim_kw, kw)\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "    final_clusters = []\n",
    "    \n",
    "    kws_ = list(kws)\n",
    "    \n",
    "    for kws_pair, cnt in Counter(pairs).items():\n",
    "        if cnt > 1:\n",
    "            final_clusters.append(kws_pair)\n",
    "            [kws_.remove(w) for w in kws_pair]\n",
    "        \n",
    "    return final_clusters + [(kw,) for kw in kws_]\n",
    "\n",
    "cluster_keywords(test_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: [('improp', 'various'), ('frown', 'dangl'), ('turn',)]\n",
      "Skipped: 194 out of 197\n",
      "Skipped: 194 out of 197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Improper conduct were variously. A dangling frowning like. Mike turned away .',\n",
       " 'Improper word were variously. And dangle frowning like. Mercer turned away .',\n",
       " 'Improperly recording were variously. The dangling frowning like. ( lord john turns away .)']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_test(keywords):\n",
    "    best_sents = []\n",
    "    clusters = cluster_keywords(keywords)\n",
    "    \n",
    "    print('Clusters:', clusters)\n",
    "    \n",
    "    for kws_tuple in clusters:\n",
    "        result = gen_sents_candidates(kws_tuple, [get_scored_phrases(kw, include_scores=False) for kw in kws_tuple])\n",
    "\n",
    "        ranked_sents = rank_sents(result, keywords)\n",
    "        \n",
    "        best_sents.append([s for s, _ in ranked_sents[:3]])\n",
    "        \n",
    "    return ['. '.join([s.capitalize() for s in ss]) for ss in zip(*best_sents)]\n",
    "\n",
    "generate_test(test_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_rules = Cache.load_terminal_rules_sents()\n",
    "\n",
    "total_sents = sum(v['S'] for v in sents_rules.values())\n",
    "\n",
    "def rank_sents(ss, kwss, top_n=30):\n",
    "    res_sents = []\n",
    "    \n",
    "    sents_stems = [[stems_map.get(vocab_ind.get(w, w), w) \n",
    "                    for w in s.split(' ')]\n",
    "                  for _, s in ss]\n",
    "    \n",
    "    if not sents_stems:\n",
    "        return []\n",
    "    \n",
    "    scores = model.score(sents_stems)\n",
    "    \n",
    "    for (s_prob, s), score in zip(ss, scores):\n",
    "        s_spl = s.split(' ')\n",
    "\n",
    "        if len(s_spl) > 2:\n",
    "            not_sws = [w\n",
    "                       for w in s_spl\n",
    "                       if w.lower() not in stop_words and stems_map[vocab_ind[w.lower()]] not in kwss] \n",
    "#             print(not_sws)\n",
    "            not_sw_prob = len(not_sws) / len(s_spl) / 10\n",
    "            \n",
    "#             print(score, np.log(s_prob or 1e-6), np.log(not_sw_prob or 1e-6))\n",
    "            \n",
    "            res_sents.append((s, score + np.log(s_prob or 1e-8) + np.log(not_sw_prob or 1e-8)))\n",
    "            \n",
    "    return sorted(res_sents, key=itemgetter(1), reverse=True)[:top_n]\n",
    "    \n",
    "def combine_elements(*args):\n",
    "    return list(product(*args, repeat=1))\n",
    "\n",
    "def gen_sents_candidates(kws, kws_phrases):\n",
    "    marks = {'COMMA': [\",\"],\n",
    "             'COLON': [\":\"],\n",
    "             'SEMICOLON': [\";\"],\n",
    "             # 'DOT': [\".\"],\n",
    "             'QUESTION': [\"?\"],\n",
    "             'EXCLAM': [\"!\"],\n",
    "             'DASH': [\"-\"]}\n",
    "    \n",
    "    res_sents = set()\n",
    "\n",
    "    if len(kws) == 1:\n",
    "        kw = kws[0]\n",
    "        phrs = kws_phrases[0]\n",
    "        \n",
    "        for p_type, p_type_sents in stems_phrases[kw].items():\n",
    "            res_sents.update((0, ' '.join(sents[s_id])) for s_ids_set in p_type_sents.values() for s_id in list(s_ids_set) )\n",
    "            \n",
    "        return res_sents\n",
    "    \n",
    "    phrases = defaultdict(dict)\n",
    "\n",
    "    for kw, kw_phrases in zip(kws, kws_phrases):\n",
    "        for p_type in kw_phrases:\n",
    "            phrases[p_type][kw] = kw_phrases[p_type]\n",
    "\n",
    "    skipped = 0\n",
    "    \n",
    "    for sents_rule, sents_count in sents_rules.items():\n",
    "        s_prob = max(sents_count['S'], 1000) / total_sents\n",
    "        \n",
    "        if s_prob < 0.03:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        sent_symbols = str2ngram(sents_rule)\n",
    "        \n",
    "        sent_phrases = []\n",
    "        \n",
    "        # form lists of phrases of each keyword\n",
    "        for p_type in sent_symbols:\n",
    "            if p_type not in marks:\n",
    "                sent_phrases.append([(kw, phrases[p_type].get(kw))\n",
    "                                     for kw in kws\n",
    "                                    if phrases[p_type].get(kw)])\n",
    "            else:\n",
    "                sent_phrases.append((None, marks.get(p_type, [])))\n",
    "            \n",
    "        # combine phrases of keywords\n",
    "        kws_phrases_product = combine_elements(*sent_phrases)\n",
    "        \n",
    "        # combine sents\n",
    "        for kws_comb in kws_phrases_product:\n",
    "            if kws_comb is None:\n",
    "                continue\n",
    "        \n",
    "            un_combs = [kw \n",
    "                        for kw, _ in kws_comb \n",
    "                        if kw is not None]\n",
    "            \n",
    "            if len(set(un_combs)) <= 1:\n",
    "                continue\n",
    "            \n",
    "            kws_comb = [comb for _, comb in kws_comb]\n",
    "            \n",
    "            sents_candidates = [(s_prob, ' '.join(s))\n",
    "                                for s in combine_elements(*kws_comb)]\n",
    "            \n",
    "            res_sents.update([s_cand for s_cand in sents_candidates])\n",
    "    \n",
    "    print('Skipped: %s out of %s' % (skipped, len(sents_rules)))\n",
    "    return res_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['effort', 'cheer', 'insult', 'occupi', 'tighter', 'far']\n",
      "\n",
      "Candidate text:\n",
      "Clusters: [('tighter', 'far'), ('effort', 'insult'), ('cheer',), ('occupi',)]\n",
      "Skipped: 194 out of 197\n",
      "Skipped: 194 out of 197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The tighter reached far. An effort insult and murder. Oh - ye - ho , cheerly men !\". Today it is occupied by the french embassy .',\n",
       " 'Tighter the reached far. Its efforts insult and murder. He was tolerably cheerful .. Otherwise , the meeting was occupied with internal administrative matters , delegates said .',\n",
       " \"Tighter the remove far. Its effort insult and murder. Good cheer , my jolly archers !. Yesterday marines and police occupied one ship , the docemarte , seamen ' s leaders said .\"]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEYWORDS = '' or gen_random_kws(6)\n",
    "\n",
    "print('Keywords:', KEYWORDS)\n",
    "\n",
    "print('\\nCandidate text:')\n",
    "\n",
    "generate_test(KEYWORDS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
