{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "\n",
    "from utils.nltk_utils import *\n",
    "from utils.ngrams import *\n",
    "from utils.eval import get_test_keywords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.grammar_utils import tags_seq_to_symbols\n",
    "from utils.io import Cache\n",
    "from utils.ngrams import ngram2str\n",
    "\n",
    "\n",
    "observed_tags = Cache.load_observed_tags()\n",
    "terminal_rules = Cache.load_terminal_rules()\n",
    "\n",
    "\n",
    "def parse_phrases(tt_ngrams) -> Tuple[List, List[List[Tuple]]]:\n",
    "    global observed_tags, terminal_rules\n",
    "    \n",
    "    phrases = []\n",
    "    phrases_types = []\n",
    "\n",
    "    if observed_tags is None:\n",
    "        observed_tags = dict()\n",
    "\n",
    "    for tt_gram in tt_ngrams:\n",
    "        symbols = tuple(tags_seq_to_symbols([tag\n",
    "                                             for _, tag in tt_gram]))\n",
    "\n",
    "        phrase = tt_gram\n",
    "\n",
    "        # check if tags phrase has been already observed\n",
    "        tags_str = ngram2str(symbols)\n",
    "\n",
    "        if tags_str in observed_tags:\n",
    "            if observed_tags[tags_str] is not None:\n",
    "                phrases.append(phrase)\n",
    "                phrases_types.append(observed_tags[tags_str])\n",
    "\n",
    "            continue\n",
    "\n",
    "        p_types_dict = terminal_rules.get(tags_str)\n",
    "\n",
    "        if p_types_dict:\n",
    "            p_type = max(p_types_dict, key=lambda k: p_types_dict[k])\n",
    "\n",
    "            phrases.append(phrase)\n",
    "            phrases_types.append(p_type)\n",
    "\n",
    "            observed_tags[tags_str] = p_type\n",
    "\n",
    "        else:\n",
    "            observed_tags[tags_str] = None\n",
    "\n",
    "#     try:\n",
    "#         Cache.save_observed_tags(observed_tags)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc(e)\n",
    "\n",
    "    return phrases_types, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents() + brown.sents() + reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 210608\n",
      "Words: 5503894\n",
      "Vocab: 87046\n"
     ]
    }
   ],
   "source": [
    "sents = [tuple(s) for s in sents]\n",
    "\n",
    "words = [w.lower() \n",
    "         for s in sents for w in s]\n",
    "\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "print('Sents:', len(sents))\n",
    "print('Words:', len(words))\n",
    "print('Vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indexes\n",
    "\n",
    "# {sentence: sent_index}\n",
    "sents_ind = {sents[i]: i for i in range(len(sents))}\n",
    "\n",
    "# {word: word_index}\n",
    "vocab_ind = {vocab[j]: j for j in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping from words to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: 58858\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in vocab]\n",
    "\n",
    "# {word_index: stem}\n",
    "stems_map = {vocab_ind[word]: stem \n",
    "             for word, stem in zip(vocab, stems)}\n",
    "\n",
    "print('Stems:', len(set(stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2word = {stem: vocab[ind]\n",
    "             for ind, stem in stems_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = gensim.models.Word2Vec.load('data/w2v/CBOW_300_10_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_stemmed = [[stems_map[vocab_ind[w.lower()]] for w in s] for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sents_stemmed, size=300, window=15, min_count=1, hs=1, negative=0)\n",
    "model.save('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.3553354740142822), ('daughter', 0.35418325662612915), ('esther', 0.3361766040325165), ('husband', 0.3246108889579773), ('absalom', 0.3200189769268036), ('mordecai', 0.31594496965408325), ('samaria', 0.3088769316673279), ('vashti', 0.30666205286979675), ('hebron', 0.298782080411911), ('selleth', 0.29791125655174255)]\n",
      "0.20989265750352995\n",
      "bring\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "print(model.wv.most_similar(positive=[stemmer.stem('woman'), stemmer.stem('king')], negative=[stemmer.stem('man')]))\n",
    "print(model.wv.similarity(stemmer.stem('campus'), stemmer.stem('dormitory')))\n",
    "print(model.wv.doesnt_match([stemmer.stem(w) for w in \"dormitory bring campus\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='rb') as fp:\n",
    "    stems_phrases = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = NGramsParser()\n",
    "\n",
    "sents_words_indexes = parser.parse_sents_tokens(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210608it [06:48, 515.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# {stem: Set[sentence_ind]}\n",
    "stems_phrases = defaultdict(dict)\n",
    "\n",
    "for i, (s_words, words_indexes) in tqdm(enumerate(zip(sents, sents_words_indexes), start=1)):\n",
    "    words_ttokens = nltk.pos_tag([w.lower() for w in s_words])\n",
    "    \n",
    "    tt_ngrams = [ngr\n",
    "                 for i in range(2, 5 + 1) \n",
    "                 for ngr in n_grams(words_ttokens, i, words_indexes, pad_left=False)]\n",
    "\n",
    "    types, phrases = parse_phrases(tt_ngrams)\n",
    "\n",
    "    # format and store phrases\n",
    "    for t, p in zip(types, phrases):\n",
    "        phrase_inds = tuple(vocab_ind[token] for token, _ in p)\n",
    "        \n",
    "        for word_ind in phrase_inds:\n",
    "            if vocab[word_ind] not in stop_words:\n",
    "                stem = stems_map[word_ind]\n",
    "                \n",
    "                if not stems_phrases[stem].get(t):\n",
    "                    stems_phrases[stem][t] = defaultdict(set)\n",
    "                \n",
    "                stem_phr_t = stems_phrases[stem][t]\n",
    "                \n",
    "                stem_phr_t[phrase_inds].add(sents_ind[s_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(word, stemmed=True):\n",
    "    if stemmed:\n",
    "        stem = word\n",
    "    else:\n",
    "        stem = stemmer.stem(word)\n",
    "        \n",
    "    phrs = stems_phrases[stem]\n",
    "    \n",
    "    if not phrs:\n",
    "        return {}\n",
    "    \n",
    "    phrases_ = defaultdict(set)\n",
    "    \n",
    "    for p_type, phr_dict in phrs.items():\n",
    "        for phrase, sents in phr_dict.items():\n",
    "            phrase_ = tuple(vocab[ind] for ind in phrase)\n",
    "            \n",
    "            phrases_[p_type].add(phrase_)\n",
    "            \n",
    "    return phrases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='wb') as fp:\n",
    "    pickle.dump(stems_phrases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for analysis\n",
    "\n",
    "phrases_count = defaultdict(dict)\n",
    "\n",
    "phrases_lengths_count = defaultdict(int)\n",
    "phrases_types_count = defaultdict(int)\n",
    "\n",
    "stems_phrases_count = defaultdict(dict)\n",
    "\n",
    "for stem, stem_phrases_dict in stems_phrases.items():\n",
    "    for phrase_type, phrases_dict in stem_phrases_dict.items():\n",
    "#         stem_phrase_type_count = sum([len(s_ids) for s_ids in phrases_dict.values()])\n",
    "        for phrase_tuple, s_ids in phrases_dict.items():\n",
    "            phrase_len = len(phrase_tuple)\n",
    "            phrase_sents_count = len(s_ids)\n",
    "            \n",
    "            if not stems_phrases_count[stem].get(phrase_len):\n",
    "                stems_phrases_count[stem][phrase_len] = defaultdict(int)\n",
    "                \n",
    "            # count number of such phrase type for given stem\n",
    "            stems_phrases_count[stem][phrase_len][phrase_type] += phrase_sents_count \n",
    "\n",
    "            # count total number of such phrase type\n",
    "            phrases_count[phrase_len][phrase_type] = phrases_count[phrase_len].get(phrase_type, 0) + phrase_sents_count\n",
    "\n",
    "            # count total number of all phrases\n",
    "            phrases_lengths_count[phrase_len] += phrase_sents_count\n",
    "            phrases_types_count[phrase_type] += phrase_sents_count\n",
    "\n",
    "all_phrases_count = sum(phrases_types_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {2: {'ADJP': 288142,\n",
       "              'ADVP': 418156,\n",
       "              'NP': 1985844,\n",
       "              'PP': 3328,\n",
       "              'VP': 66991},\n",
       "             3: {'ADJP': 44770,\n",
       "              'ADVP': 31064,\n",
       "              'NP': 1353053,\n",
       "              'PP': 7554,\n",
       "              'VP': 8215},\n",
       "             4: {'ADJP': 4179, 'ADVP': 223, 'NP': 429226},\n",
       "             5: {'ADJP': 245, 'NP': 53025}})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: defaultdict(int,\n",
       "             {'ADJP': 170, 'ADVP': 1833, 'NP': 5259, 'PP': 32, 'VP': 12}),\n",
       " 3: defaultdict(int, {'ADJP': 13, 'ADVP': 4, 'NP': 3858, 'PP': 30}),\n",
       " 4: defaultdict(int, {'NP': 879}),\n",
       " 5: defaultdict(int, {'NP': 50})}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = 'share'\n",
    "\n",
    "stems_phrases_count[stemmer.stem(WORD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NP', 2), -6.7941026773674675),\n",
       " (('NP', 3), -7.1038946574911375),\n",
       " (('ADVP', 2), -7.848089603117106),\n",
       " (('NP', 4), -8.583013953271635),\n",
       " (('ADJP', 2), -10.226000413906549),\n",
       " (('NP', 5), -11.449775845528665),\n",
       " (('PP', 2), -11.896062948157084),\n",
       " (('PP', 3), -11.960601469294655),\n",
       " (('ADJP', 3), -12.796849493495273),\n",
       " (('VP', 2), -12.87689220116881),\n",
       " (('ADVP', 3), -13.975504489836922)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_phrases(word, stemmed=True):\n",
    "    \"\"\"\n",
    "        Return sorted log probs of phrase types and length for the word \n",
    "    \"\"\"\n",
    "    if not stemmed:\n",
    "        stem = stemmer.stem(word)\n",
    "    else:\n",
    "        stem = word\n",
    "    \n",
    "    if not stems_phrases_count.get(stem):\n",
    "        return []\n",
    "    \n",
    "    stem_phrases = stems_phrases_count[stem]\n",
    "    \n",
    "    phrases_probs = []\n",
    "    \n",
    "    for p_length, p_types_count in stem_phrases.items():\n",
    "        for p_type, p_type_count in p_types_count.items():\n",
    "            prob_len = phrases_lengths_count[p_length] / all_phrases_count\n",
    "            \n",
    "            prob_len_type = phrases_count[p_length][p_type] / phrases_lengths_count[p_length]\n",
    "            \n",
    "            prob_word_len_type = p_type_count / phrases_count[p_length][p_type]\n",
    "            \n",
    "            log_prob = sum(np.log(p) for p in [prob_word_len_type, prob_len_type, prob_len])\n",
    "            \n",
    "            phrases_probs.append(((p_type, p_length), log_prob))\n",
    "    \n",
    "    phrases, scores = [], []\n",
    "    for phr, s in sorted(phrases_probs, key=itemgetter(1), reverse=True):\n",
    "        phrases.append(phr)\n",
    "        scores.append(s)\n",
    "    \n",
    "#     scores = np.exp(scores)\n",
    "#     scores /= np.max(scores)\n",
    "    \n",
    "    return list(zip(phrases, scores))\n",
    "\n",
    "get_word_phrases(WORD, stemmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_PEN = {\n",
    "    2: 1.75,\n",
    "    3: 1.25,\n",
    "    4: 0.75,\n",
    "    5: 0.25\n",
    "}\n",
    "P_TYPE_SCORE = dict(get_word_phrases(WORD, stemmed=False))\n",
    "REVERSE = False\n",
    "SCORE_SIGN = -1\n",
    "TOP_N = 5\n",
    "\n",
    "phrases_ = get_phrases(WORD, stemmed=False)\n",
    "\n",
    "all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "for p_type, phrases in phrases_.items():\n",
    "    phrases_ = list(phrases)\n",
    "    \n",
    "    scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                   for phr in phrases_]))\n",
    "\n",
    "    for phr, sc in zip(phrases_, scores):\n",
    "        if not all_scored_phrases[p_type].get(len(phr)):\n",
    "            all_scored_phrases[p_type][len(phr)] = []\n",
    "        \n",
    "        score = SCORE_SIGN * (sc + P_TYPE_SCORE[(p_type, len(phr))]) * LENGTH_PEN[len(phr)] \n",
    "        \n",
    "        all_scored_phrases[p_type][len(phr)].append((phr, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP\n",
      "\n",
      "2\n",
      "[('per share', 15.479596212664338),\n",
      " ('outstanding share', 17.877954676905368),\n",
      " ('outstanding shares', 17.877954676905368),\n",
      " ('shares outstanding', 17.877954676905368),\n",
      " ('share outstanding', 17.877954676905368)]\n",
      "\n",
      "3\n",
      "[('per share figure', 14.207894735781464),\n",
      " ('common share outstanding', 15.331119232622894),\n",
      " ('outstanding common share', 15.331119828669342),\n",
      " ('outstanding common shares', 15.331119828669342),\n",
      " ('share tender offer', 16.137549810854704)]\n",
      "\n",
      "4\n",
      "[('a share tender offer', 13.806982354449577),\n",
      " ('its outstanding common shares', 13.993565396594352),\n",
      " ('the outstanding common shares', 14.166183070468254),\n",
      " ('class b common share', 14.340641812609977),\n",
      " ('class b common shares', 14.340641812609977)]\n",
      "\n",
      "5\n",
      "[('a share cash tender offer', 6.792060174227014),\n",
      " ('all the outstanding common shares', 6.843968428850427),\n",
      " ('share cash tender offer today', 7.618016757250086),\n",
      " ('all common and preference shares', 7.650655783891931),\n",
      " ('a share cash takeover offer', 7.808050669908777)]\n",
      "\n",
      "ADVP\n",
      "\n",
      "2\n",
      "[('per share', 17.324073332726208),\n",
      " ('of share', 25.63853557723686),\n",
      " ('sharing of', 25.638537246166912),\n",
      " ('share of', 25.638537246166912),\n",
      " ('for share', 26.89914436954185)]\n",
      "\n",
      "3\n",
      "[('nil per share', 34.81757354251771), ('ago per share', 38.967383618277594)]\n",
      "\n",
      "VP\n",
      "\n",
      "2\n",
      "[('sharing in', 38.3437633078407),\n",
      " ('sharing at', 38.440713123590825),\n",
      " ('sharing with', 47.79925008228528)]\n",
      "\n",
      "PP\n",
      "\n",
      "2\n",
      "[('to share', 38.1422900470465), ('to sharing', 38.1422900470465)]\n",
      "\n",
      "3\n",
      "[('of its shares', 31.2967415407504),\n",
      " ('for their shares', 32.995268876871),\n",
      " ('of their shares', 33.82620769294156),\n",
      " ('on its shares', 33.974691207727325),\n",
      " ('that its shares', 39.49053006919278)]\n",
      "\n",
      "ADJP\n",
      "\n",
      "2\n",
      "[('share offered', 26.56781954052146),\n",
      " ('share converted', 31.566116516610933),\n",
      " ('share held', 32.0553107905596),\n",
      " ('share earned', 32.205990140458894),\n",
      " ('shared in', 33.704702680131746)]\n",
      "\n",
      "3\n",
      "[('purolator share outstanding', 31.798835104097485),\n",
      " ('share dividend payable', 32.68091589747822),\n",
      " ('share repurchase last', 41.04823977290547),\n",
      " ('share plus accrued', 42.21758992015278),\n",
      " ('share and multiple', 42.9320946389333)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    print('%s\\n' % p_type)\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "        print(n)\n",
    "        pprint(best_phr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ADJP': [('share offered', 26.56781954052146),\n",
       "              ('share converted', 31.566116516610933),\n",
       "              ('purolator share outstanding', 31.798835104097485),\n",
       "              ('share held', 32.0553107905596),\n",
       "              ('share earned', 32.205990140458894),\n",
       "              ('share dividend payable', 32.68091589747822),\n",
       "              ('shared in', 33.704702680131746),\n",
       "              ('share repurchase last', 41.04823977290547),\n",
       "              ('share plus accrued', 42.21758992015278),\n",
       "              ('share and multiple', 42.9320946389333)],\n",
       "             'ADVP': [('per share', 17.324073332726208),\n",
       "              ('of share', 25.63853557723686),\n",
       "              ('sharing of', 25.638537246166912),\n",
       "              ('share of', 25.638537246166912),\n",
       "              ('for share', 26.89914436954185),\n",
       "              ('nil per share', 34.81757354251771),\n",
       "              ('ago per share', 38.967383618277594)],\n",
       "             'NP': [('a share cash tender offer', 6.792060174227014),\n",
       "              ('all the outstanding common shares', 6.843968428850427),\n",
       "              ('share cash tender offer today', 7.618016757250086),\n",
       "              ('all common and preference shares', 7.650655783891931),\n",
       "              ('a share cash takeover offer', 7.808050669908777),\n",
       "              ('a share tender offer', 13.806982354449577),\n",
       "              ('its outstanding common shares', 13.993565396594352),\n",
       "              ('the outstanding common shares', 14.166183070468254),\n",
       "              ('per share figure', 14.207894735781464),\n",
       "              ('class b common share', 14.340641812609977),\n",
       "              ('class b common shares', 14.340641812609977),\n",
       "              ('common share outstanding', 15.331119232622894),\n",
       "              ('outstanding common share', 15.331119828669342),\n",
       "              ('outstanding common shares', 15.331119828669342),\n",
       "              ('per share', 15.479596212664338),\n",
       "              ('share tender offer', 16.137549810854704),\n",
       "              ('outstanding share', 17.877954676905368),\n",
       "              ('outstanding shares', 17.877954676905368),\n",
       "              ('shares outstanding', 17.877954676905368),\n",
       "              ('share outstanding', 17.877954676905368)],\n",
       "             'PP': [('of its shares', 31.2967415407504),\n",
       "              ('for their shares', 32.995268876871),\n",
       "              ('of their shares', 33.82620769294156),\n",
       "              ('on its shares', 33.974691207727325),\n",
       "              ('to share', 38.1422900470465),\n",
       "              ('to sharing', 38.1422900470465),\n",
       "              ('that its shares', 39.49053006919278)],\n",
       "             'VP': [('sharing in', 38.3437633078407),\n",
       "              ('sharing at', 38.440713123590825),\n",
       "              ('sharing with', 47.79925008228528)]})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group different length together\n",
    "\n",
    "candidate_phrases = defaultdict(list)\n",
    "\n",
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=REVERSE)[:TOP_N]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "        \n",
    "        candidate_phrases[p_type].extend(best_phr)\n",
    "        \n",
    "    candidate_phrases[p_type] = sorted(candidate_phrases[p_type], key=itemgetter(1), reverse=REVERSE)\n",
    "    \n",
    "candidate_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  874\n"
     ]
    }
   ],
   "source": [
    "kws = get_test_keywords('data/lingualeo_words.csv')\n",
    "\n",
    "print('Keywords: ', len(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test keywords: ['advice', 'drab', 'shame', 'proclivity', 'admissible']\n",
      "Test keywords stems: ['advic', 'drab', 'shame', 'procliv', 'admiss']\n",
      "All kws found: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5\n",
    "np.random.seed = 0\n",
    "\n",
    "all_kws_found = False\n",
    "\n",
    "while not all_kws_found:    \n",
    "    test_kws = list(np.random.choice(list(kws), size=TEST_SIZE, replace=False))\n",
    "\n",
    "    print('Test keywords:', test_kws)\n",
    "\n",
    "    test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "    kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "    print('Test keywords stems:', test_kws)\n",
    "\n",
    "    all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    print('All kws found:', all_kws_found)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advic [1.0484983  0.5876609  0.67798024 0.6448846 ]\n",
      "0 1\n",
      "advic shame\n",
      "drab [1.0484983  0.85772705 0.5455078  0.85652786]\n",
      "1 2\n",
      "drab procliv\n",
      "shame [0.5876609  0.85772705 0.9682493  0.80564225]\n",
      "2 0\n",
      "shame advic\n",
      "procliv [0.67798024 0.5455078  0.9682493  0.66142946]\n",
      "3 1\n",
      "procliv drab\n",
      "admiss [0.6448846  0.85652786 0.8056423  0.66142946]\n",
      "4 0\n",
      "admiss advic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({('admiss', 'advic'): 1,\n",
       "         ('advic', 'shame'): 2,\n",
       "         ('drab', 'procliv'): 2})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pairs_counter(test_kws):\n",
    "    pairs = []\n",
    "\n",
    "    for i, kw in enumerate(test_kws):\n",
    "        cp = list(test_kws)\n",
    "        cp.remove(kw)\n",
    "\n",
    "        dsts = w2v.distances(kw, cp)\n",
    "        print(kw, dsts)\n",
    "        max_ind = np.argmin(dsts)\n",
    "        print(i, max_ind)\n",
    "        sim_kw = test_kws[max_ind + (1 if i <= max_ind else 0)]\n",
    "        print(kw, sim_kw)\n",
    "\n",
    "        pair = (kw, sim_kw)\n",
    "\n",
    "        if (sim_kw, kw) in pairs:\n",
    "            pair = (sim_kw, kw)\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "    return Counter(pairs)\n",
    "\n",
    "get_pairs_counter(test_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using processed corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "t = 'grievance'\n",
    "\n",
    "s = stemmer.stem(t)\n",
    "# stems_phrases[s]\n",
    "vocab_ind['grievance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "t = 'grievance'\n",
    "\n",
    "s = stemmer.stem(t)\n",
    "\n",
    "ss = []\n",
    "for k, v in stems_phrases[s]['NP'].items():\n",
    "    ss.extend([s_id for s_id in v])\n",
    "    \n",
    "Counter(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
