{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "\n",
    "from utils.nltk_utils import *\n",
    "from utils.ngrams import *\n",
    "from utils.grammar import parse_phrases\n",
    "from utils.eval import get_test_keywords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.grammar_utils import tags_seq_to_symbols\n",
    "from utils.io import Cache\n",
    "from utils.ngrams import ngram2str\n",
    "\n",
    "\n",
    "observed_tags = Cache.load_observed_tags()\n",
    "terminal_rules = Cache.load_terminal_rules()\n",
    "\n",
    "\n",
    "def parse_phrases(tt_ngrams) -> Tuple[List, List[List[Tuple]]]:\n",
    "    global observed_tags, terminal_rules\n",
    "    \n",
    "    phrases = []\n",
    "    phrases_types = []\n",
    "\n",
    "    if observed_tags is None:\n",
    "        observed_tags = dict()\n",
    "\n",
    "    for tt_gram in tt_ngrams:\n",
    "        symbols = tuple(tags_seq_to_symbols([tag\n",
    "                                             for _, tag in tt_gram]))\n",
    "\n",
    "        phrase = tt_gram\n",
    "\n",
    "        # check if tags phrase has been already observed\n",
    "        tags_str = ngram2str(symbols)\n",
    "\n",
    "        if tags_str in observed_tags:\n",
    "            if observed_tags[tags_str] is not None:\n",
    "                phrases.append(phrase)\n",
    "                phrases_types.append(observed_tags[tags_str])\n",
    "\n",
    "            continue\n",
    "\n",
    "        p_types_dict = terminal_rules.get(tags_str)\n",
    "\n",
    "        if p_types_dict:\n",
    "            p_type = max(p_types_dict, key=lambda k: p_types_dict[k])\n",
    "\n",
    "            phrases.append(phrase)\n",
    "            phrases_types.append(p_type)\n",
    "\n",
    "            observed_tags[tags_str] = p_type\n",
    "\n",
    "        else:\n",
    "            observed_tags[tags_str] = None\n",
    "\n",
    "#     try:\n",
    "#         Cache.save_observed_tags(observed_tags)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc(e)\n",
    "\n",
    "    return phrases_types, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents() + brown.sents() + reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 210608\n",
      "Words: 5503894\n",
      "Vocab: 87046\n"
     ]
    }
   ],
   "source": [
    "sents = [tuple(s) for s in sents]\n",
    "\n",
    "words = [w.lower() \n",
    "         for s in sents for w in s]\n",
    "\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "print('Sents:', len(sents))\n",
    "print('Words:', len(words))\n",
    "print('Vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indexes\n",
    "\n",
    "# {sentence: sent_index}\n",
    "sents_ind = {sents[i]: i for i in range(len(sents))}\n",
    "\n",
    "# {word: word_index}\n",
    "vocab_ind = {vocab[j]: j for j in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping from words to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: 58858\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in vocab]\n",
    "\n",
    "# {word_index: stem}\n",
    "stems_map = {vocab_ind[word]: stem \n",
    "             for word, stem in zip(vocab, stems)}\n",
    "\n",
    "print('Stems:', len(set(stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2word = {stem: vocab[ind]\n",
    "             for ind, stem in stems_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='rb') as fp:\n",
    "    stems_phrases = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = NGramsParser()\n",
    "\n",
    "sents_words_indexes = parser.parse_sents_tokens(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210608it [07:18, 480.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# {stem: Set[sentence_ind]}\n",
    "stems_phrases = defaultdict(dict)\n",
    "\n",
    "for i, (s_words, words_indexes) in tqdm(enumerate(zip(sents, sents_words_indexes), start=1)):\n",
    "    words_ttokens = nltk.pos_tag([w.lower() for w in s_words])\n",
    "    \n",
    "    tt_ngrams = [ngr\n",
    "                 for i in range(1, 5 + 1) \n",
    "                 for ngr in n_grams(words_ttokens, i, words_indexes, pad_left=False)]\n",
    "\n",
    "    types, phrases = parse_phrases(tt_ngrams)\n",
    "\n",
    "    # format and store phrases\n",
    "    for t, p in zip(types, phrases):\n",
    "        phrase_inds = tuple(vocab_ind[token] for token, _ in p)\n",
    "        \n",
    "        for word_ind in phrase_inds:\n",
    "            if vocab[word_ind] not in stop_words:\n",
    "                stem = stems_map[word_ind]\n",
    "                \n",
    "                if not stems_phrases[stem].get(t):\n",
    "                    stems_phrases[stem][t] = defaultdict(set)\n",
    "                \n",
    "                stem_phr_t = stems_phrases[stem][t]\n",
    "                \n",
    "                stem_phr_t[phrase_inds].add(sents_ind[s_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(word, stemmed=True):\n",
    "    if stemmed:\n",
    "        stem = word\n",
    "    else:\n",
    "        stem = stemmer.stem(word)\n",
    "        \n",
    "    phrs = stems_phrases[stem]\n",
    "    \n",
    "    if not phrs:\n",
    "        return {}\n",
    "    \n",
    "    phrases_ = defaultdict(set)\n",
    "    \n",
    "    for p_type, phr_dict in phrs.items():\n",
    "        for phrase, sents in phr_dict.items():\n",
    "            phrase_ = tuple(vocab[ind] for ind in phrase)\n",
    "            \n",
    "            phrases_[p_type].add(phrase_)\n",
    "            \n",
    "    return phrases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "with open('data/cache/stems_phrases', mode='wb') as fp:\n",
    "    pickle.dump(stems_phrases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = gensim.models.Word2Vec.load('data/w2v/CBOW_300_10_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_stemmed = [[stems_map[vocab_ind[w.lower()]] for w in s] for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sents_stemmed, size=300, window=15, min_count=1, hs=1, negative=0)\n",
    "model.save('data/w2v/CBOW_300_10_hs_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.3553354740142822), ('daughter', 0.35418325662612915), ('esther', 0.3361766040325165), ('husband', 0.3246108889579773), ('absalom', 0.3200189769268036), ('mordecai', 0.31594496965408325), ('samaria', 0.3088769316673279), ('vashti', 0.30666205286979675), ('hebron', 0.298782080411911), ('selleth', 0.29791125655174255)]\n",
      "0.20989265750352995\n",
      "bring\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "print(model.wv.most_similar(positive=[stemmer.stem('woman'), stemmer.stem('king')], negative=[stemmer.stem('man')]))\n",
    "print(model.wv.similarity(stemmer.stem('campus'), stemmer.stem('dormitory')))\n",
    "print(model.wv.doesnt_match([stemmer.stem(w) for w in \"dormitory bring campus\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for analysis\n",
    "\n",
    "phrases_count = defaultdict(dict)\n",
    "\n",
    "phrases_lengths_count = defaultdict(int)\n",
    "phrases_types_count = defaultdict(int)\n",
    "\n",
    "stems_phrases_count = defaultdict(dict)\n",
    "\n",
    "for stem, stem_phrases_dict in stems_phrases.items():\n",
    "    for phrase_type, phrases_dict in stem_phrases_dict.items():\n",
    "#         stem_phrase_type_count = sum([len(s_ids) for s_ids in phrases_dict.values()])\n",
    "        for phrase_tuple, s_ids in phrases_dict.items():\n",
    "            phrase_len = len(phrase_tuple)\n",
    "            phrase_sents_count = len(s_ids)\n",
    "            \n",
    "            if not stems_phrases_count[stem].get(phrase_len):\n",
    "                stems_phrases_count[stem][phrase_len] = defaultdict(int)\n",
    "                \n",
    "            # count number of such phrase type for given stem\n",
    "            stems_phrases_count[stem][phrase_len][phrase_type] += phrase_sents_count \n",
    "\n",
    "            # count total number of such phrase type\n",
    "            phrases_count[phrase_len][phrase_type] = phrases_count[phrase_len].get(phrase_type, 0) + phrase_sents_count\n",
    "\n",
    "            # count total number of all phrases\n",
    "            phrases_lengths_count[phrase_len] += phrase_sents_count\n",
    "            phrases_types_count[phrase_type] += phrase_sents_count\n",
    "\n",
    "all_phrases_count = sum(phrases_types_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {1: {'ADJP': 332093,\n",
       "              'ADVP': 131822,\n",
       "              'NP': 1396926,\n",
       "              'PP': 41047,\n",
       "              'VP': 550396},\n",
       "             2: {'ADJP': 288142,\n",
       "              'ADVP': 418156,\n",
       "              'NP': 1985844,\n",
       "              'PP': 3328,\n",
       "              'VP': 66991},\n",
       "             3: {'ADJP': 44770,\n",
       "              'ADVP': 31064,\n",
       "              'NP': 1353053,\n",
       "              'PP': 7554,\n",
       "              'VP': 8215},\n",
       "             4: {'ADJP': 4179, 'ADVP': 223, 'NP': 429226},\n",
       "             5: {'ADJP': 245, 'NP': 53025}})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: defaultdict(int, {'ADJP': 4, 'ADVP': 2, 'NP': 4, 'PP': 3, 'VP': 304}),\n",
       " 2: defaultdict(int, {'ADJP': 42, 'ADVP': 4, 'NP': 116, 'VP': 6}),\n",
       " 3: defaultdict(int, {'NP': 27, 'VP': 1}),\n",
       " 4: defaultdict(int, {'NP': 7}),\n",
       " 5: defaultdict(int, {'NP': 1})}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_phrases_count['assum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('VP', 1), -10.065077456873636),\n",
       " (('NP', 2), -11.028514967173495),\n",
       " (('ADJP', 2), -12.044435539996488),\n",
       " (('NP', 3), -12.486268292275527),\n",
       " (('NP', 4), -13.836195009224543),\n",
       " (('VP', 2), -13.990345689051804),\n",
       " (('NP', 1), -14.395810797159967),\n",
       " (('ADJP', 1), -14.395810797159967),\n",
       " (('ADVP', 2), -14.395810797159967),\n",
       " (('PP', 1), -14.683492869611747),\n",
       " (('ADVP', 1), -15.088957977719911),\n",
       " (('VP', 3), -15.782105158279858),\n",
       " (('NP', 5), -15.782105158279858)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_phrases(word, stemmed=True):\n",
    "    \"\"\"\n",
    "        Return sorted log probs of phrase types and length for the word \n",
    "    \"\"\"\n",
    "    if not stemmed:\n",
    "        stem = stemmer.stem(word)\n",
    "    else:\n",
    "        stem = word\n",
    "    \n",
    "    if not stems_phrases_count.get(stem):\n",
    "        return []\n",
    "    \n",
    "    stem_phrases = stems_phrases_count[stem]\n",
    "    \n",
    "    phrases_probs = []\n",
    "    \n",
    "    for p_length, p_types_count in stem_phrases.items():\n",
    "        for p_type, p_type_count in p_types_count.items():\n",
    "            prob_len = phrases_lengths_count[p_length] / all_phrases_count\n",
    "            \n",
    "            prob_len_type = phrases_count[p_length][p_type] / phrases_lengths_count[p_length]\n",
    "            \n",
    "            prob_word_len_type = p_type_count / phrases_count[p_length][p_type]\n",
    "            \n",
    "            log_prob = sum(np.log(p) for p in [prob_word_len_type, prob_len_type, prob_len])\n",
    "            \n",
    "            phrases_probs.append(((p_type, p_length), log_prob))\n",
    "    \n",
    "    return sorted(phrases_probs, key=itemgetter(1), reverse=True)\n",
    "\n",
    "get_word_phrases('assume', stemmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_ = get_phrases('assume', stemmed=False)\n",
    "\n",
    "all_scored_phrases = defaultdict(dict)\n",
    "\n",
    "for p_type, phrases in phrases_.items():\n",
    "    phrases_ = list(phrases)\n",
    "    \n",
    "    scores = np.array(model.score([[stemmer.stem(w) for w in phr] \n",
    "                                   for phr in phrases_]))\n",
    "#     scores = np.exp(scores - np.max(scores, axis=0))\n",
    "#     scores = np.log(scores / scores.sum(axis=0))\n",
    "    for phr, sc in zip(phrases_, scores):\n",
    "        if not all_scored_phrases[p_type].get(len(phr)):\n",
    "            all_scored_phrases[p_type][len(phr)] = []\n",
    "    \n",
    "        all_scored_phrases[p_type][len(phr)].append((phr, sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP\n",
      "\n",
      "1\n",
      "[('assumes', -9.87307), ('assume', -9.87307)]\n",
      "\n",
      "2\n",
      "[('that assumed', -9.019429),\n",
      " ('assume that', -9.019429),\n",
      " ('assume man', -9.951341),\n",
      " ('assume office', -10.356435),\n",
      " ('assume the', -11.597148)]\n",
      "\n",
      "3\n",
      "[('assuming crude oil', -16.74822),\n",
      " ('assuming oil prices', -17.29162),\n",
      " ('an assumed air', -17.8685),\n",
      " ('assuming market conditions', -17.922554),\n",
      " ('the question assumed', -19.673552)]\n",
      "\n",
      "4\n",
      "[('an assumed air of', -21.883614),\n",
      " ('with an assumed air', -22.628468),\n",
      " ('and an assumed impact', -25.068672),\n",
      " ('under an assumed name', -25.50867),\n",
      " ('an assumed impact velocity', -29.45615)]\n",
      "\n",
      "5\n",
      "[('assumes some other horrible forme', -39.57282)]\n",
      "\n",
      "ADVP\n",
      "\n",
      "1\n",
      "[('assumes', -9.87307), ('assume', -9.87307)]\n",
      "\n",
      "2\n",
      "[('assume the', -11.597148),\n",
      " ('this assumes', -15.635136),\n",
      " ('shrs assume', -21.739788),\n",
      " ('there assumes', -22.380066)]\n",
      "\n",
      "VP\n",
      "\n",
      "1\n",
      "[('assumes', -9.87307),\n",
      " ('assumed', -9.87307),\n",
      " ('assume', -9.87307),\n",
      " ('assuming', -9.87307)]\n",
      "\n",
      "2\n",
      "[('assuming that', -9.019429),\n",
      " ('assuming for', -17.421629),\n",
      " ('assumed alma', -26.23574)]\n",
      "\n",
      "3\n",
      "[('assume & lt', -25.785336)]\n",
      "\n",
      "PP\n",
      "\n",
      "1\n",
      "[('assume', -9.87307)]\n",
      "\n",
      "ADJP\n",
      "\n",
      "1\n",
      "[('assumed', -9.87307)]\n",
      "\n",
      "2\n",
      "[('assumed that', -9.019429),\n",
      " ('not assuming', -12.990057),\n",
      " ('debt assumed', -13.382556),\n",
      " ('certain assumed', -16.211546),\n",
      " ('assumed by', -16.679369)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p_type, scored_phrases in all_scored_phrases.items():\n",
    "    print('%s\\n' % p_type)\n",
    "    for n in scored_phrases:\n",
    "        nps = scored_phrases[n]\n",
    "        best_phr = sorted(nps, key=itemgetter(1), reverse=True)[:5]\n",
    "\n",
    "        best_phr = [(' '.join(p), s) for p, s in best_phr]\n",
    "\n",
    "        print(n)\n",
    "        pprint(best_phr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[('advice', -10.578289)]\n",
      "\n",
      "2\n",
      "[('my advice', -10.6507635),\n",
      " ('give advice', -11.619597),\n",
      " ('wanted advice', -13.039914),\n",
      " ('and advice', -13.891079),\n",
      " ('him advice', -13.957809)]\n",
      "\n",
      "3\n",
      "[('such advice as', -12.637161),\n",
      " ('of the advice', -18.26074),\n",
      " ('the advice of', -18.26074),\n",
      " ('their advice for', -18.533184),\n",
      " ('their advice .\"', -18.64547)]\n",
      "\n",
      "4\n",
      "[('for some other advice', -23.814346),\n",
      " ('your advice and counsel', -25.596104),\n",
      " ('her kindness and advice', -27.856691),\n",
      " ('the free advice of', -28.051847),\n",
      " (\"the hetman's advice of\", -29.096107)]\n",
      "\n",
      "5\n",
      "[('the close-in support and advice', -41.098873)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5 + 1):\n",
    "    nps = scored_nps[i]\n",
    "    best_nps = sorted(nps, key=itemgetter(1), reverse=True)[:5]\n",
    "    \n",
    "    best_nps = [(' '.join(p), s) for p, s in best_nps]\n",
    "    \n",
    "    print(i)\n",
    "    pprint(best_nps)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  874\n"
     ]
    }
   ],
   "source": [
    "kws = get_test_keywords('data/lingualeo_words.csv')\n",
    "\n",
    "print('Keywords: ', len(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test keywords: ['advice', 'drab', 'shame', 'proclivity', 'admissible']\n",
      "Test keywords stems: ['advic', 'drab', 'shame', 'procliv', 'admiss']\n",
      "All kws found: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5\n",
    "np.random.seed = 0\n",
    "\n",
    "all_kws_found = False\n",
    "\n",
    "while not all_kws_found:    \n",
    "    test_kws = list(np.random.choice(list(kws), size=TEST_SIZE, replace=False))\n",
    "\n",
    "    print('Test keywords:', test_kws)\n",
    "\n",
    "    test_kws = [stemmer.stem(kw) for kw in test_kws]\n",
    "    kws_phrases = [stems_phrases.get(kw) for kw in test_kws]\n",
    "    print('Test keywords stems:', test_kws)\n",
    "\n",
    "    all_kws_found = all(kws_phrases)\n",
    "    \n",
    "    print('All kws found:', all_kws_found)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advic [1.0484983  0.5876609  0.67798024 0.6448846 ]\n",
      "0 1\n",
      "advic shame\n",
      "drab [1.0484983  0.85772705 0.5455078  0.85652786]\n",
      "1 2\n",
      "drab procliv\n",
      "shame [0.5876609  0.85772705 0.9682493  0.80564225]\n",
      "2 0\n",
      "shame advic\n",
      "procliv [0.67798024 0.5455078  0.9682493  0.66142946]\n",
      "3 1\n",
      "procliv drab\n",
      "admiss [0.6448846  0.85652786 0.8056423  0.66142946]\n",
      "4 0\n",
      "admiss advic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({('admiss', 'advic'): 1,\n",
       "         ('advic', 'shame'): 2,\n",
       "         ('drab', 'procliv'): 2})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pairs_counter(test_kws):\n",
    "    pairs = []\n",
    "\n",
    "    for i, kw in enumerate(test_kws):\n",
    "        cp = list(test_kws)\n",
    "        cp.remove(kw)\n",
    "\n",
    "        dsts = w2v.distances(kw, cp)\n",
    "        print(kw, dsts)\n",
    "        max_ind = np.argmin(dsts)\n",
    "        print(i, max_ind)\n",
    "        sim_kw = test_kws[max_ind + (1 if i <= max_ind else 0)]\n",
    "        print(kw, sim_kw)\n",
    "\n",
    "        pair = (kw, sim_kw)\n",
    "\n",
    "        if (sim_kw, kw) in pairs:\n",
    "            pair = (sim_kw, kw)\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "    return Counter(pairs)\n",
    "\n",
    "get_pairs_counter(test_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using processed corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "t = 'grievance'\n",
    "\n",
    "s = stemmer.stem(t)\n",
    "# stems_phrases[s]\n",
    "vocab_ind['grievance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "t = 'grievance'\n",
    "\n",
    "s = stemmer.stem(t)\n",
    "\n",
    "ss = []\n",
    "for k, v in stems_phrases[s]['NP'].items():\n",
    "    ss.extend([s_id for s_id in v])\n",
    "    \n",
    "Counter(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
