{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, brown, reuters\n",
    "import nltk\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 155892\n"
     ]
    }
   ],
   "source": [
    "fileids = None\n",
    "# fileids = brown.fileids()[:250]\n",
    "sents_brown = list(brown.sents(fileids=fileids))\n",
    "\n",
    "fileids = None\n",
    "# fileids = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', '']\n",
    "sents_gutenberg = list(gutenberg.sents(fileids=fileids))\n",
    "\n",
    "sents = sents_brown + sents_gutenberg\n",
    "\n",
    "print('Sentences:', len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"tuple\") to list",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8a6eccfcceb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<START>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<END>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msents_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindx_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"tuple\") to list"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for i in range(len(sents)):\n",
    "    sents[i] = tuple(['<START>'] + sents[i] + ['<END>'])\n",
    "    \n",
    "sents_indx = {i: s for i, s in enumerate(sents)}\n",
    "indx_sents = {s: i for i, s in enumerate(sents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 84544\n"
     ]
    }
   ],
   "source": [
    "words_sents = defaultdict(set)\n",
    "\n",
    "for i, s in sents_indx.items():\n",
    "    for w in s:\n",
    "        words_sents[w].add(i)\n",
    "\n",
    "print('Unique words:', len(words_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sents(word):\n",
    "    return [sents_indx[i] for i in words_sents.get(word, set())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_sents(words):\n",
    "    if isinstance(words, str):\n",
    "        words = [w.strip() for w in words.split(',')]\n",
    "    \n",
    "    text = []\n",
    "\n",
    "    for w in words:\n",
    "        w_sents = word_sents(w)\n",
    "\n",
    "        if w_sents:\n",
    "            s = random.choice(w_sents)\n",
    "        else:\n",
    "            s = tuple()\n",
    "\n",
    "        text.append(' '.join(s))\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START> When driving rain or mist socked in one valley , Fogg would '\n",
      " 'chandelle up and over to reverse course and try another one , ranging from '\n",
      " 'the Ottauquechee up to Danville in search of safe passage through the '\n",
      " 'mountain passes . <END>',\n",
      " '<START> 19 : 34 But the stranger that dwelleth with you shall be unto you as '\n",
      " 'one born among you , and thou shalt love him as thyself ; for ye were '\n",
      " 'strangers in the land of Egypt : I am the LORD your God . <END>',\n",
      " '<START> The swords caught on each other with a dreadful clang and jar , full '\n",
      " 'of the old energy and hate ; and at once plunged and replunged . <END>']\n"
     ]
    }
   ],
   "source": [
    "pprint(generate_text_sents('try, love, hate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Dictogram(dict):\n",
    "    def __init__(self, iterable=None):\n",
    "        # Инициализируем наше распределение как новый объект класса, \n",
    "        # добавляем имеющиеся элементы\n",
    "        super(Dictogram, self).__init__()\n",
    "        self.types = 0  # число уникальных ключей в распределении\n",
    "        self.tokens = 0  # общее количество всех слов в распределении\n",
    "        if iterable:\n",
    "            self.update(iterable)\n",
    "\n",
    "    def update(self, iterable):\n",
    "        # Обновляем распределение элементами из имеющегося \n",
    "        # итерируемого набора данных\n",
    "        for item in iterable:\n",
    "            if item in self:\n",
    "                self[item] += 1\n",
    "                self.tokens += 1\n",
    "            else:\n",
    "                self[item] = 1\n",
    "                self.types += 1\n",
    "                self.tokens += 1\n",
    "\n",
    "    def count(self, item):\n",
    "        # Возвращаем значение счетчика элемента, или 0\n",
    "        if item in self:\n",
    "            return self[item]\n",
    "        return 0\n",
    "\n",
    "    def return_random_word(self):\n",
    "        random_key = random.sample(self, 1)\n",
    "        # Другой способ:\n",
    "        # random.choice(histogram.keys())\n",
    "        return random_key[0]\n",
    "\n",
    "    def return_weighted_random_word(self):\n",
    "        # Сгенерировать псевдослучайное число между 0 и (n-1),\n",
    "        # где n - общее число слов\n",
    "        random_int = random.randint(0, self.tokens-1)\n",
    "        index = 0\n",
    "        list_of_keys = list(self.keys())\n",
    "        # вывести 'случайный индекс:', random_int\n",
    "        for i in range(0, self.types):\n",
    "            index += self[list_of_keys[i]]\n",
    "            # вывести индекс\n",
    "            if(index > random_int):\n",
    "                # вывести list_of_keys[i]\n",
    "                return list_of_keys[i]\n",
    "            \n",
    "\n",
    "def make_markov_model(data):\n",
    "    markov_model = dict()\n",
    "\n",
    "    for i in range(0, len(data)-1):\n",
    "        if data[i] in markov_model:\n",
    "            # Просто присоединяем к уже существующему распределению\n",
    "            markov_model[data[i]].update([data[i+1]])\n",
    "        else:\n",
    "            markov_model[data[i]] = Dictogram([data[i+1]])\n",
    "            \n",
    "    return markov_model\n",
    "\n",
    "def make_higher_order_markov_model(order, data):\n",
    "    markov_model = dict()\n",
    "\n",
    "    for i in range(0, len(data)-order):\n",
    "        # Создаем окно\n",
    "        window = tuple(data[i: i+order])\n",
    "        # Добавляем в словарь\n",
    "        if window in markov_model:\n",
    "            # Присоединяем к уже существующему распределению\n",
    "            markov_model[window].update([data[i+order]])\n",
    "        else:\n",
    "            markov_model[window] = Dictogram([data[i+order]])\n",
    "    return markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "def generate_random_start(model):\n",
    "    # Чтобы сгенерировать любое начальное слово, раскомментируйте строку:\n",
    "    # return random.choice(model.keys())\n",
    "\n",
    "    # Чтобы сгенерировать \"правильное\" начальное слово, используйте код ниже:\n",
    "    # Правильные начальные слова - это те, что являлись началом предложений в корпусе\n",
    "    if '<END>' in model:\n",
    "        seed_word = '<END>'\n",
    "        while seed_word == '<END>':\n",
    "            seed_word = model['<END>'].return_weighted_random_word()\n",
    "        return seed_word\n",
    "    return random.choice(list(model.keys()))\n",
    "\n",
    "\n",
    "def generate_random_sentence(length, markov_model):\n",
    "    current_word = generate_random_start(markov_model)\n",
    "    sentence = [current_word]\n",
    "    for i in range(0, length):\n",
    "        current_dictogram = markov_model[current_word]\n",
    "        random_weighted_word = current_dictogram.return_weighted_random_word()\n",
    "        current_word = random_weighted_word\n",
    "        sentence.append(current_word)\n",
    "#     sentence[0] = sentence[0].capitalize()\n",
    "#     return ' '.join(sentence) + '.'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "\n",
    "sents_ngrams = []\n",
    "\n",
    "for s in sents:\n",
    "    sents_ngrams.append(\n",
    "        nltk.ngrams(s, n=n, pad_left=False, pad_right=False, left_pad_symbol='START', right_pad_symbol='END')\n",
    "    )\n",
    "    \n",
    "text_ngrams = [ngram for sent_ngrams in sents_ngrams for ngram in sent_ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_model = make_markov_model([str(ngr[0]) for ngr in text_ngrams[:1000000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> Does a people into visiting `` places cooperative must have restricted to put . <END> <START> Other scientific explanations are\n"
     ]
    }
   ],
   "source": [
    "s = generate_random_sentence(20, m_model)\n",
    "print(' '.join(s))\n",
    "sent = []\n",
    "\n",
    "for ngr in s:\n",
    "    for w in ngr:\n",
    "        if w not in sent:\n",
    "            sent.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams: 4406545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(None, None, '<START>'),\n",
       " (None, '<START>', 'The'),\n",
       " ('<START>', 'The', 'Fulton'),\n",
       " ('The', 'Fulton', 'County'),\n",
       " ('Fulton', 'County', 'Grand'),\n",
       " ('County', 'Grand', 'Jury'),\n",
       " ('Grand', 'Jury', 'said'),\n",
       " ('Jury', 'said', 'Friday'),\n",
       " ('said', 'Friday', 'an'),\n",
       " ('Friday', 'an', 'investigation'),\n",
       " ('an', 'investigation', 'of'),\n",
       " ('investigation', 'of', \"Atlanta's\"),\n",
       " ('of', \"Atlanta's\", 'recent'),\n",
       " (\"Atlanta's\", 'recent', 'primary'),\n",
       " ('recent', 'primary', 'election'),\n",
       " ('primary', 'election', 'produced'),\n",
       " ('election', 'produced', '``'),\n",
       " ('produced', '``', 'no'),\n",
       " ('``', 'no', 'evidence'),\n",
       " ('no', 'evidence', \"''\"),\n",
       " ('evidence', \"''\", 'that'),\n",
       " (\"''\", 'that', 'any'),\n",
       " ('that', 'any', 'irregularities'),\n",
       " ('any', 'irregularities', 'took'),\n",
       " ('irregularities', 'took', 'place'),\n",
       " ('took', 'place', '.'),\n",
       " ('place', '.', '<END>'),\n",
       " ('.', '<END>', None),\n",
       " ('<END>', None, None),\n",
       " (None, None, '<START>'),\n",
       " (None, '<START>', 'The'),\n",
       " ('<START>', 'The', 'jury'),\n",
       " ('The', 'jury', 'further'),\n",
       " ('jury', 'further', 'said'),\n",
       " ('further', 'said', 'in'),\n",
       " ('said', 'in', 'term-end'),\n",
       " ('in', 'term-end', 'presentments'),\n",
       " ('term-end', 'presentments', 'that'),\n",
       " ('presentments', 'that', 'the'),\n",
       " ('that', 'the', 'City'),\n",
       " ('the', 'City', 'Executive'),\n",
       " ('City', 'Executive', 'Committee'),\n",
       " ('Executive', 'Committee', ','),\n",
       " ('Committee', ',', 'which'),\n",
       " (',', 'which', 'had'),\n",
       " ('which', 'had', 'over-all'),\n",
       " ('had', 'over-all', 'charge'),\n",
       " ('over-all', 'charge', 'of'),\n",
       " ('charge', 'of', 'the'),\n",
       " ('of', 'the', 'election'),\n",
       " ('the', 'election', ','),\n",
       " ('election', ',', '``'),\n",
       " (',', '``', 'deserves'),\n",
       " ('``', 'deserves', 'the'),\n",
       " ('deserves', 'the', 'praise'),\n",
       " ('the', 'praise', 'and'),\n",
       " ('praise', 'and', 'thanks'),\n",
       " ('and', 'thanks', 'of'),\n",
       " ('thanks', 'of', 'the'),\n",
       " ('of', 'the', 'City'),\n",
       " ('the', 'City', 'of'),\n",
       " ('City', 'of', 'Atlanta'),\n",
       " ('of', 'Atlanta', \"''\"),\n",
       " ('Atlanta', \"''\", 'for'),\n",
       " (\"''\", 'for', 'the'),\n",
       " ('for', 'the', 'manner'),\n",
       " ('the', 'manner', 'in'),\n",
       " ('manner', 'in', 'which'),\n",
       " ('in', 'which', 'the'),\n",
       " ('which', 'the', 'election'),\n",
       " ('the', 'election', 'was'),\n",
       " ('election', 'was', 'conducted'),\n",
       " ('was', 'conducted', '.'),\n",
       " ('conducted', '.', '<END>'),\n",
       " ('.', '<END>', None),\n",
       " ('<END>', None, None),\n",
       " (None, None, '<START>'),\n",
       " (None, '<START>', 'The'),\n",
       " ('<START>', 'The', 'September-October'),\n",
       " ('The', 'September-October', 'term'),\n",
       " ('September-October', 'term', 'jury'),\n",
       " ('term', 'jury', 'had'),\n",
       " ('jury', 'had', 'been'),\n",
       " ('had', 'been', 'charged'),\n",
       " ('been', 'charged', 'by'),\n",
       " ('charged', 'by', 'Fulton'),\n",
       " ('by', 'Fulton', 'Superior'),\n",
       " ('Fulton', 'Superior', 'Court'),\n",
       " ('Superior', 'Court', 'Judge'),\n",
       " ('Court', 'Judge', 'Durwood'),\n",
       " ('Judge', 'Durwood', 'Pye'),\n",
       " ('Durwood', 'Pye', 'to'),\n",
       " ('Pye', 'to', 'investigate'),\n",
       " ('to', 'investigate', 'reports'),\n",
       " ('investigate', 'reports', 'of'),\n",
       " ('reports', 'of', 'possible'),\n",
       " ('of', 'possible', '``'),\n",
       " ('possible', '``', 'irregularities'),\n",
       " ('``', 'irregularities', \"''\"),\n",
       " ('irregularities', \"''\", 'in')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('N-grams:', len(text_ngrams))\n",
    "text_ngrams[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
